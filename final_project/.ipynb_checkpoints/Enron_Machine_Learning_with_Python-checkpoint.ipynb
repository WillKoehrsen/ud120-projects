{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Enron Person of Interest Identification </h1> </center>\n",
    "<center> <h4> Udacity Data Analyst Nanodegree P5 </h4> </center>\n",
    "\n",
    "<h4> Dataset Background </h4>\n",
    "\n",
    "The Enron email + financial dataset is a large trove of information regarding the Enron Corporation, an energy, commodities, and services company that infamously went bankrupt in December 2001 as a result of fradulent business practices. In the aftermath of the company's collapse, the Federal Energy Regulatory Commission released more 1.6 million emails sent and received by Enron executives in the years from 2000-2002 ([History of Enron](https://en.wikipedia.org/wiki/Enron). After numerous complaints regarding the sensitive nature of the emails, the FERC redacted a large portion of the emails, but about 0.5 million remained [available to the public](https://www.technologyreview.com/s/515801/the-immortal-life-of-the-enron-e-mails/). The email + financial data contains all of the emails, meta data about the emails such as number received and sent for each individual, and financial information such as salary and stock options. (The dataset, along with several functions used in this report, is available on [Udacity's GitHub](https://github.com/udacity/ud120-projects).) The dataset has become a playground for machine learning practicioners eager to try to develop models that can identify persons of interests, defined as an individual who was eventually tried for fraud in the Enron investigation. The goal of this project was to develop a model that could pick out the POI's. I choose not to use the text of the email's themselves, but rather the metadata about the emails as well as the financial information. The ultimate objective of investigating the Enron dataset is to be able to predict cases of fraud or unsafe business practices far in advance, so those responsible can be punished, and those who are innocent are not harmed. Machine learning holds the promise to a world where there are no more Enrons, but let's not get ahead of ourselves just yet! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Outlier Investigation </h2>\n",
    "\n",
    "The first step is to load in the data and investigate it for any errors or outliers that need to be corrected or removed. The data is provided in the form of a dictionary, and I will convert it to a pandas dataframe for easier data manipulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "payment_data = ['salary',\n",
    "                'bonus',\n",
    "                'long_term_incentive',\n",
    "                'deferred_income',\n",
    "                'deferral_payments',\n",
    "                'loan_advances',\n",
    "                'other',\n",
    "                'expenses',                \n",
    "                'director_fees', \n",
    "                'total_payments']\n",
    "\n",
    "stock_data = ['exercised_stock_options',\n",
    "              'restricted_stock',\n",
    "              'restricted_stock_deferred',\n",
    "              'total_stock_value']\n",
    "\n",
    "email_data = ['to_messages',\n",
    "              'from_messages',\n",
    "              'from_poi_to_this_person',\n",
    "              'from_this_person_to_poi',\n",
    "              'shared_receipt_with_poi']\n",
    "              \n",
    "              \n",
    "features_list = ['poi'] + payment_data + stock_data + email_data\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = df.replace('NaN', np.nan)\n",
    "df = df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 20 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "dtypes: bool(1), float64(19)\n",
      "memory usage: 23.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the info about the dataset, I can see that all the fields are floating point numbers except for the poi identification which is True/False. There are 146 rows in the dataframe which most likely mean there are 146 individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous NaNs in both the email and financial fields. According to the [official pdf documentation for the finacial (payment and stock) data](https://github.com/udacity/ud120-projects/blob/master/final_project/enron61702insiderpay.pdf), values of NaN represent 0 and not unknown quantities. However, for the email data, NaNs stand for unknown information. Therefore, I will replace any financial data that is NaN with 0 but will fill in the NaNs for the email data with the mean of the column grouped by person of interest. In other words, if a person has a NaN value for 'to_messages', and they are a person of interest, I will fill in that value with the mean value of 'to_messages' for a person of interest. If I chose to drop the NaNs, that would reduce the size of what is already a small dataset. As the quality of a machine learning model is proportional to the amount of data fed into it, I am hesitant to remove any information that could possibly be of use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[payment_data] = df[payment_data].fillna(0)\n",
    "df[stock_data] = df[stock_data].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wkoehrse\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\n",
    "\n",
    "df_poi = df[df['poi'] == True]\n",
    "df_nonpoi = df[df['poi']==False]\n",
    "\n",
    "df_poi.ix[:, email_data] = imp.fit_transform(df_poi.ix[:,email_data]);\n",
    "df_nonpoi.ix[:, email_data] = imp.fit_transform(df_nonpoi.ix[:,email_data]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_poi.append(df_nonpoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to check for outliers/incorrect data is to add up all of the payment related columns for each person and see if that is equal to the total payment recorded for the individual. I can also do the same for stock payments. If the data was entered by hand, I would expect that there may be a few errors that I can correct by comparing to the official PDF prepared by FineLaw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "      <td>2007.111111</td>\n",
       "      <td>668.763889</td>\n",
       "      <td>58.5</td>\n",
       "      <td>36.277778</td>\n",
       "      <td>1058.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>463.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False     0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY  False     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT              -44093.0  2007.111111     668.763889   \n",
       "BHATNAGAR SANJAY                0.0   523.000000      29.000000   \n",
       "\n",
       "                  from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "BELFER ROBERT                        58.5                36.277778   \n",
       "BHATNAGAR SANJAY                      0.0                 1.000000   \n",
       "\n",
       "                  shared_receipt_with_poi  \n",
       "BELFER ROBERT                 1058.527778  \n",
       "BHATNAGAR SANJAY               463.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[payment_data[:-1]].sum(axis='columns') != df['total_payments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, there are two individuals for which the sum of their different payments does not add up to the recorded total payment. \n",
    "The errors appear to be caused by a misalignment of the columns; for Robert Belfer, the financial data has been shifted one column to the right, and for Sanjay Bhatnagar, the data has been shifted one column to the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the incorrect data for Belfer\n",
    "belfer_financial = df.ix['BELFER ROBERT', 1:15].tolist()\n",
    "# Delete the first element to shift left and add on a 0 to end as indicated in financial data\n",
    "belfer_financial.pop(0)\n",
    "belfer_financial.append(0)\n",
    "# Reinsert corrected data\n",
    "df.ix['BELFER ROBERT', 1:15] = belfer_financial\n",
    "\n",
    "# Retrieve the incorrect data for Bhatnagar\n",
    "bhatnagar_financial = df.ix['BHATNAGAR SANJAY', 1:15].tolist()\n",
    "# Delete the last element to shift right and add on a 0 to beginning\n",
    "bhatnagar_financial.pop(-1)\n",
    "bhatnagar_financial = [0] + bhatnagar_financial\n",
    "# Reinsert corrected data\n",
    "df.ix['BHATNAGAR SANJAY', 1:15] = bhatnagar_financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[payment_data[:-1]].sum(axis='columns') != df['total_payments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[stock_data[:-1]].sum(axis='columns') != df['total_stock_value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting the shifted financial data eliminated two errors. However, there may still be outliers in the dataset that need to be removed. Looking through the official financial PDF, I can see that I need to remove 'TOTAL' as it is currently the last row of the dataframe and it will throw off any predictions (even though this is correct data, it is not a person and will be of no value when trying to identify persons of interest). Likewise, there is an entry for 'THE TRAVEL AGENCY IN THE PARK', which according to the documentation, was a company co-owned by Enron's former Chairman's sister and is clearly not an individual that should be included in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=['TOTAL','THE TRAVEL AGENCY IN THE PARK'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now look for outlying data points recorded in the different fields. However, I will be conservative in terms of removing the outliers because the dataset is rather small for machine learning in the first place. Moreover, the outliers might actually be important as they could represent patterns in the data that would aid in the identification of persons of interest. The [official definition of a mild outlier](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm) is either below the (first quartile  minus 1.5 times the Interquartile Range (IQR))  or  above the (third quartile plus 1.5 times the IQR). My approach will be to count the number of outlying features each individual has in the dataset. I will then investigate these outliers to determine if any should be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IQR = df.quantile(q=0.75) - df.quantile(q=0.25)\n",
    "first_quartile = df.quantile(q=0.25)\n",
    "third_quartile = df.quantile(q=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAY KENNETH L         15\n",
       "FREVERT MARK A        12\n",
       "BELDEN TIMOTHY N       9\n",
       "SKILLING JEFFREY K     9\n",
       "BAXTER JOHN C          8\n",
       "LAVORATO JOHN J        8\n",
       "DELAINEY DAVID W       7\n",
       "KEAN STEVEN J          7\n",
       "HAEDICKE MARK E        7\n",
       "WHALLEY LAWRENCE G     7\n",
       "RICE KENNETH D         6\n",
       "KITCHEN LOUISE         6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = df[(df>(third_quartile + 1.5*IQR) ) | (df<(first_quartile - 1.5*IQR) )].count(axis=1)\n",
    "outliers.sort_values(axis=0, ascending=False, inplace=True)\n",
    "outliers.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this point, I need to do some research before blinding deleting outliers. Based on the small number of persons of interest initially in the dataset, I decided to not remove any individuals who are persons are interest regardless of the number of outliers they may have. An outlier could be a sign of fradulent activity, as it could be evidence that someone is laundering illegal funds through the company payroll or perhaps an accomplice is being paid to remain silent. I will examine several of the top outlying inviduals out of curiousity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few interesting observations about the outliers:\n",
    "\n",
    "1. Kenneth Lay, [the CEO of Enron from 1986-2001](http://www.biography.com/people/kenneth-lay-234611), presided over many of the illegal business activites and hence is one of the most important persons of interest. \n",
    "2. Mark Frevert served as chief executive of [Enron Europe from 1986-2000 and was appointed as chairman of Enron in 2001](http://www.risk.net/risk-management/2123422/ten-years-after-its-collapse-enron-lives-energy-markets). He was a major player in the firm, although not a person of interest. I believe that he is not representative of the average employee at Enron during this time because of his substantial compensation and will remove him from the dataset. \n",
    "3. Timothy Belden was the [former head of trading for Enron](http://articles.latimes.com/2007/feb/15/business/fi-enron15) who developed the strategy to illegally raise energy prices in California. He was a person of interest and will definitely remain in the dataset. \n",
    "4. Jeffrey Skilling [replaced Kenneth Lay as CEO of Enron in 2001 and orchestrated much of the fraud](http://www.biography.com/people/jeffrey-skilling-235386) that destroyed Enron. As a person of interest, he will remain in the dataset. \n",
    "5. John Baxter was a former vice Enron vice chairman and [died of an apparent self-inflicted gunshot](https://www.wsws.org/en/articles/2002/01/enro-j28.html) before he was able to testify against other Enron executives. I will remove him from the dataset as he is not a person of interest. \n",
    "6. John Lavorato was a top executive in the energy-trading branch of Enron and received large bonuses to [keep him from leaving Enron](http://www.nytimes.com/2002/06/18/business/officials-got-a-windfall-before-enron-s-collapse.html). As he was not a person of interest, and the large bonus ended up skewing his total pay towards the top of the range, I think it would be appropriate to remove him from the dataset. \n",
    "7. Lawrence Whalley [served as the president of Enron](http://www.corpwatch.org/article.php?id=13194) and fired Andrew Fastow once it was apparent the severity of Enron's situation. He was investigated thoroughly but not identified as a person of interest and therefore will be removed from the dataset.  \n",
    "\n",
    "Total, I decided to remove four people from the dataset. I believe these removals are justified primarily because none of these individuals were persons of interest and they all were upper-level executives with pay levels far above the average employee. I do not think these top executives who did not commit fraud are indicative of the majority of employees at Enron who also did not commit fraud (i.e. they were not persons of interest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=['FREVERT MARK A', 'LAVORATO JOHN J', 'WHALLEY LAWRENCE G', 'BAXTER JOHN C'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    122\n",
       "True      18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df==0].count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 2800 observations of financial and email data in the set now that the data cleaning has been finished. Of these, __1150 or 41%__ are 0 for financial (payment and stock) values. There are 18 persons of interest, comprising __12.9%__ of the individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> First Algorithm Testing and Performance Metrics </h2>\n",
    "\n",
    "I now want to try training some algorithms on all of the initial features in the dataset to gauge the importance of features and to see what the accuracy is like without any parameter tuning. \n",
    "The four algorithms I have selected for initial testing are Gaussian Naive Bayes (GaussianNB), DecisionTreeClassifier, Support Vector Classifier (SVC), and KMeans Clustering. I will run the algorithms with the default parameters except I will alter the kernel used in the Support Vector Machine to be linear. I will also select number of clusters = 2 for KMeans as I know in advance that there are two categories that should be classified by the model. Although accuracy would appear to be the obvious choice for evaluating the quality of a classifier, accuracy can be a crude measure at times, and is not suited for some datasets including this one. For example, if a classifier were to guess that all of the samples in the cleaned dataset were _not_ persons of interest, it would have an accuracy of 87.1%. However, this clearly would not satisfy the objective of this investigation which is to create a classifier that can identify _persons of interest_. Therefore, different metrics are needed to evaluate the tuned algorithm to gauge its effectiveness. The two selected for this project are [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "* __Precision__ is the number of correct positive classifications divided by the total number of positive labels assigned. In other words, it is the fraction of persons of interest predicted by the algorithm that are truly persons of interest.  Mathematically precision is defined as \n",
    "\n",
    "$$ precision = \\frac{true\\ positives}{true\\ positives + false\\ positives} $$ \n",
    "\n",
    "* __Recall__ is the number of correct positive classifications divided by the number of positive instances that should have been identified. In other words, it is the fraction of the total number of persons of interest that the classifier identifies. Mathematically, recall is defined as\n",
    "\n",
    "$$ recall = \\frac{true\\ positives}{true\\ positives + false\\ negatives} $$ \n",
    "\n",
    "Precision is also known as positive predictive value while recall is the sensitivity of the classifier. A combined measured of precision and recall is the [__F1 score__](https://en.wikipedia.org/wiki/F1_score). Is it the harmonic mean of precision and recall. Mathematically, the F1 score is defined as:\n",
    "\n",
    "$$ F1\\ Score = \\frac{2\\ (precision\\ x\\ recall)}{precision + recall} $$\n",
    "\n",
    "For this project, the objective was a precision and a recall both greater than 0.3. However, I believe it is possible to do much better than that with the right feature selection and algorithm tuning. \n",
    "\n",
    "<h4> Scaling </h4>\n",
    "\n",
    "The only preparation I will do for this initial testing of the different algorithms is to scale the data such that it has a zero mean and a unit variance. This process is called [normalization](http://www.analytictech.com/ba762/handouts/normalization.htm) and is accomplished using the scale function from the sklearn preprocessing module. Scaling of some form (whether that is MinMax scaling or normalization) is usually necessary because there are different units for the features in the dataset and this process is able to create non-dimensional features. If the features are not scaled, those with a larger units can have an undue influence on the classifier, especially if the classifier uses some sort of measurement (such as Euclidean distance) in order to make predictions. [Here is a good dicussion of feature scaling and normalization](https://stats.stackexchange.com/questions/121886/when-should-i-apply-feature-scaling-for-my-data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.70714\tPrecision: 0.30909\tRecall: 0.85000\tF1: 0.45333\tF2: 0.62963\n",
      "\tTotal predictions:  140\tTrue positives:   17\tFalse positives:   38\tFalse negatives:    3\tTrue negatives:   82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import tester\n",
    "\n",
    "# Scale the dataset and send it back to a dictionary\n",
    "scaled_df = df.copy()\n",
    "scaled_df.ix[:,1:] = scale(scaled_df.ix[:,1:])\n",
    "my_dataset = scaled_df.to_dict(orient='index')\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.83571\tPrecision: 0.43478\tRecall: 0.50000\tF1: 0.46512\tF2: 0.48544\n",
      "\tTotal predictions:  140\tTrue positives:   10\tFalse positives:   13\tFalse negatives:   10\tTrue negatives:  107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.86429\tPrecision: 0.57143\tRecall: 0.20000\tF1: 0.29630\tF2: 0.22989\n",
      "\tTotal predictions:  140\tTrue positives:    4\tFalse positives:    3\tFalse negatives:   16\tTrue negatives:  117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.79286\tPrecision: 0.09091\tRecall: 0.05000\tF1: 0.06452\tF2: 0.05495\n",
      "\tTotal predictions:  140\tTrue positives:    1\tFalse positives:   10\tFalse negatives:   19\tTrue negatives:  110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from running the four classifiers on the entire featureset with no algorithm tuning are summarized in the table below\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.30909   | 0.85000 | 0.45333  | 0.70714  |\n",
    "| DecisionTree          | 0.5000    | 0.45000 | 0.47368  | 0.85714  |\n",
    "| SVC (kernel='linear') | 0.57143   | 0.2000  | 0.29630  | 0.86429  |\n",
    "| KMeans (n_clusters=2) | 0.17647   | 0.15000 | 0.16216  | 0.77857  |\n",
    "\n",
    "From the first run through the four algorithms, I can see that the decision tree performed best, followed by the gaussian naive bayes, support vector machine, and Kmeans clustering. In fact, the decision tree and naive Bayes classifiers both perform well enough to meet the standards for the project. Nonetheless, there is much work that can be done to improve these metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> A Quick Note on Validation </h2> \n",
    "\n",
    "The validation strategy I am using here is a form of cross-validation that is implemented in the provided tester.py script. [Cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) performs multiple splits on the dataset and in each split, forms a different training and testing set. Each iteration, the classifier is fit on a training set and then tested on a testing set. The next iteration the classifier is again trained and tested, but on different sets and this process continues for the number of splits made of the dataset. Cross-validation prevents me from making the classic mistake of training an algoithm on the same data used to test the algorithm. If this happens, the test results may show that the classifier is accurate, but that is only because the algorithm has seen the testing data before. When the classifier is deployed on novel samples, the performance may be poor because it was trained and tuned for a very specific set of instances. The classifier will not be able to generalize to new cases becuase it is only fit and tuned to the specific samples it is trained and tested on. Cross-validation solves this issue by training and testing on multiple different subsets of the features and labels and is ideal for use on small datasets. Throughout my analysis, I used cross-validation to assess the performance of my algorithms. The tester.py script uses the [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) cross-validation method, and GridSearchCV, which is used to find the optimal number of features and the best parameters employs cross validation with the StratifiedKFolds method. In both cases, the number of splits of the dataset is 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "Although the decision tree algorithm already performs above the standard for precision and recall, I want to create new features that could possibly improve performance. I will also need to carry out feature selection to remove those features that are not useful for predicting a person of interest. \n",
    "\n",
    "After thinking over the background of the Enron case and the information contained in the dataset, I thought of three new features to create from the email meta data. The first will be the ratio of emails to an individual from a person of interest to all emails addressed to that person, the second is the same but for messages to persons of interest, and the third will be the ratio of email receipts shared with a person of interest to all emails addressed to that individual. The rational behind these choices is that the absolute number of emails from or to a person of interest might not matter so much as the relative number when scaled by the total emails an individual sends or receives. My instinct says that individuals who interact more with a person of interest are themselves more likely to be a person of interest because the fraud was not perpertrated alone and required a net of individuals. However, there are also some innocent persons who may have sent or received many emails from persons of interest simply in the course of their daily (perfectly above the table) work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['to_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['from_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "df['shared_poi_ratio'] = df['shared_receipt_with_poi'] / df['to_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list.append('to_poi_ratio')\n",
    "features_list.append('from_poi_ratio')\n",
    "features_list.append('shared_poi_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I will also create new features using the financial data. I have a few theories that I formed from my initial data exploration and reading about the Enron case. I think that people recieving large bonuses may be more likely to be persons of interest becuase the bonuses could be a result of fraudulent activity, or perhaps a bribe to keep someone quiet. Whatever the case may be, I will create two new features that are the bonus in relation to the salary, and the bonus in relation to total payments. There are now a total of 25 features, some of which are most likely redudant or not of any value. However, I will perform feature reduction/selection eventually so I am not worried about the large number of features. Moreover the algorithms I am using are able to train relatively quickly even with the large number of features because the total amount of data samples is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['bonus_to_salary'] = df['bonus'] / df['salary']\n",
    "df['bonus_to_total'] = df['bonus'] / df['total_payments']\n",
    "features_list.append('bonus_to_salary')\n",
    "features_list.append('bonus_to_total')                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding in the additional five features, I want to retest all of the algorithms. I will use the same tester.py script that implements cross validation to assess the precision and recall of the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.72857\tPrecision: 0.32000\tRecall: 0.80000\tF1: 0.45714\tF2: 0.61538\n",
      "\tTotal predictions:  140\tTrue positives:   16\tFalse positives:   34\tFalse negatives:    4\tTrue negatives:   86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill any NaN financial data with a 0\n",
    "df.fillna(value= 0, inplace=True)\n",
    "\n",
    "# Create a copy of the dataframe and normalize it to zero mean and unit variance\n",
    "scaled_df = df.copy()\n",
    "scaled_df.ix[:,1:] = scale(scaled_df.ix[:,1:])\n",
    "\n",
    "# Send the dataset from dataframe to dictionary for tester.py\n",
    "my_dataset = scaled_df.to_dict(orient='index')\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.90714\tPrecision: 0.68421\tRecall: 0.65000\tF1: 0.66667\tF2: 0.65657\n",
      "\tTotal predictions:  140\tTrue positives:   13\tFalse positives:    6\tFalse negatives:    7\tTrue negatives:  114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.83571\tPrecision: 0.33333\tRecall: 0.15000\tF1: 0.20690\tF2: 0.16854\n",
      "\tTotal predictions:  140\tTrue positives:    3\tFalse positives:    6\tFalse negatives:   17\tTrue negatives:  114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding in the features, the results for all of the algorithms improved and are summarized below:\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.35556   | 0.80000 | 0.49321  | 0.76429  |\n",
    "| DecisionTree          | 0.60000   | 0.60000 | 0.60000  | 0.88571  |\n",
    "| SVC (kernel='linear') | 0.71429 | 0.25000 | 0.37037  | 0.87857  |\n",
    "| KMeans (n_clusters=2) | 0.12500   | 0.25000 | 0.16667  | 0.64286  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score for the decision tree is stil the highest followed by the Gaussian Naive Bayes. At this point, neither the SVC with the linear kernel nor the KMeans clustering pass the standards of 0.3 for precision and recall. I will drop the latter two algorithms and I will also drop the GaussianNB in favor of [AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf) because I want to experiment with tunable parameters and has GaussianNB has none. AdaBoost takes a weak classifier, and trains it multiple times on a dataset, each run adjusting the weights of incorrectly classified instances to concentrate on the most difficult to classify samples. AdaBoost works on top on a weak existing classifier in an iterative manner and can be used in conjuction with the DecisionTree or GaussianNB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Selection </h1> \n",
    "\n",
    "There are several methods available for performing feature selection. One is simply to look at the feature importances for a classifier and modify the list of features to exclude those with an importance below a chosen threshold. Another is to use [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) and have the k-best features automatically selected for use in the classifier. I will look at the feature importances for both the DecisionTree and the AdaBoost Classifier, but I would prefer to use SelectKBest to actually choose the features to keep. Additionally, I can use [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) in combination with SelectKBest to find the optimal number of features to use. This will run through a number of k values and choose the one that yields the highest value according to a performance metric. \n",
    "\n",
    "First, I will manually look at the feature importances for both classifiers to get a sense of which features are most important. One of the neat concepts about machine learning is that it can help humans to think smarter. By looking at what the algorithm chooses as the most important features to identify persons of interest, it can inform humans what they should be looking for in similar cases. (One example of this in the real world is when Google's AlphaGo defeated Lee Sedol in Go, it advanced the entire state of Go by providing new [insights and strategies](https://www.youtube.com/watch?v=dTGthmNmrK4) into a game humans have played for millenia.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.90000\tPrecision: 0.65000\tRecall: 0.65000\tF1: 0.65000\tF2: 0.65000\n",
      "\tTotal predictions:  140\tTrue positives:   13\tFalse positives:    7\tFalse negatives:    7\tTrue negatives:  113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree = tester.test_classifier(clf_tree, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Feature Importances:\n",
      "\n",
      "from_poi_ratio : 0.3782\n",
      "shared_receipt_with_poi : 0.3150\n",
      "expenses : 0.2476\n",
      "to_poi_ratio : 0.0592\n",
      "salary : 0.0000\n",
      "bonus : 0.0000\n",
      "long_term_incentive : 0.0000\n",
      "deferred_income : 0.0000\n",
      "deferral_payments : 0.0000\n",
      "loan_advances : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances of the DecisionTree Classifier\n",
    "tree_feature_importances = (clf_tree.feature_importances_)\n",
    "tree_features = zip(tree_feature_importances, features_list[1:])\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "# Display the feature names and importance values\n",
    "print('Tree Feature Importances:\\n')\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.94286\tPrecision: 0.83333\tRecall: 0.75000\tF1: 0.78947\tF2: 0.76531\n",
      "\tTotal predictions:  140\tTrue positives:   15\tFalse positives:    3\tFalse negatives:    5\tTrue negatives:  117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ada = AdaBoostClassifier()\n",
    "clf_ada = tester.test_classifier(clf_ada, my_dataset, features_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Feature Importances:\n",
      "\n",
      "shared_receipt_with_poi : 0.1200\n",
      "exercised_stock_options : 0.1000\n",
      "from_this_person_to_poi : 0.1000\n",
      "to_poi_ratio : 0.1000\n",
      "deferred_income : 0.0800\n",
      "from_messages : 0.0800\n",
      "from_poi_ratio : 0.0800\n",
      "other : 0.0600\n",
      "total_stock_value : 0.0600\n",
      "shared_poi_ratio : 0.0400\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances for the AdaBoost Classifier\n",
    "ada_feature_importances = clf_ada.feature_importances_\n",
    "ada_features = zip(ada_feature_importances, features_list[1:])\n",
    "\n",
    "# Display the feature names and importance values\n",
    "print('Ada Boost Feature Importances:\\n')\n",
    "ada_features = sorted(ada_features, key=lambda x:x[0], reverse=True)\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(ada_features[i][1], ada_features[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is interesting to compare the feature importances for the DecisionTree and the AdaBoost classifiers. The top 10 features are not in close agreement for the top ten even though both classifiers achieve a respectable F1 Score greater than 0.5. However, rather than manually selecting the features to keep, I will use GridSearchCV to find the optimal number of features for the classifiers. GridSearchCV runs through a parameter grid and tests all the different configurations provided to it. It returns the parameters that yield the maximum score. I will use a scoring parameter of F1 because that is what I would like to maximize, and a cross-validation with 10 splits to ensure that I am not overfitting the algorithm to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "data_dict = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wkoehrse\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_features = np.arange(1, len(features_list))\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "tree_clf= GridSearchCV(pipe, param_grid=param_grid, scoring='f1', cv = 10)\n",
    "tree_clf.fit(features, labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 18}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the grid search performed with SelectKBest with the number of features ranging from 1 to the number of features, the __optimal number of features for the decision tree classifier is 19__. I can look at the scores assigned to the top performing features using the scores attribute of SelectKBest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Feature F-statistics:\n",
      "\n",
      "bonus : 36.2022\n",
      "from_poi_ratio : 25.8246\n",
      "salary : 25.4609\n",
      "total_stock_value : 24.8372\n",
      "exercised_stock_options : 24.0888\n",
      "bonus_to_total : 20.8908\n",
      "deferred_income : 17.0963\n",
      "bonus_to_salary : 17.0251\n",
      "shared_poi_ratio : 16.1635\n",
      "shared_receipt_with_poi : 14.7385\n",
      "from_poi_to_this_person : 12.9213\n",
      "long_term_incentive : 12.7397\n",
      "total_payments : 10.2104\n",
      "restricted_stock : 10.0832\n",
      "other : 7.9157\n",
      "loan_advances : 7.0635\n",
      "expenses : 5.7008\n",
      "from_this_person_to_poi : 3.4099\n",
      "to_poi_ratio : 2.8564\n"
     ]
    }
   ],
   "source": [
    "tree_selection = SelectKBest(k=19)\n",
    "tree_selection.fit_transform(features, labels)\n",
    "\n",
    "tree_scores = tree_selection.scores_\n",
    "tree_features = zip(tree_scores, features_list[1:])\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "print('Tree Feature F-statistics:\\n')\n",
    "for i in range(19):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SelectKBest defaults to scoring parameters using the [ANOVA F-value](http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-analysis-of-variance-anova-and-the-f-test) which is a measure of variation between sample means. It describes how much of the variance between labels is explained by a particular feature. A higher value therefore means that there is more variation in that feature between person of interests and non persons of interest. The following table summarizes the Decision Tree features and the ANOVA F-Value returned by SelectKBest with k = 19. These are the features I used in my final DecisionTreeClassifier. Here are the features as well as their f-statistic score:\n",
    "\n",
    "| Feature                 | F-statistic |\n",
    "|-------------------------|-------------|\n",
    "| bonus                   | 36.2        |\n",
    "| from_poi_ratio          | 25.8        |\n",
    "| salary                  | 25.5        |\n",
    "| total_stock_value       | 24.8        |\n",
    "| exercised_stock_options | 24.1        |\n",
    "| bonus_to_total          | 20.9        |\n",
    "| deferred_income         | 17.1        |\n",
    "| bonus_to_salary         | 17.0        |\n",
    "| shared_poi_ratio        | 16.2        |\n",
    "| shared_receipt_with_poi | 14.7        |\n",
    "| from_poi_to_this_person | 12.9        |\n",
    "| long_term_incentive     | 12.7        |\n",
    "| total_payments          | 10.2        |\n",
    "| restricted_stock        | 10.1        |\n",
    "| other                   | 7.92        |\n",
    "| loan_advances           | 7.06        |\n",
    "| expenses                | 5.70        |\n",
    "| from_this_person_to_poi | 3.41        |\n",
    "| to_poi_ratio            | 2.86        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.90714\tPrecision: 0.66667\tRecall: 0.70000\tF1: 0.68293\tF2: 0.69307\n",
      "\tTotal predictions:  140\tTrue positives:   14\tFalse positives:    7\tFalse negatives:    6\tTrue negatives:  113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "tester.dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the DecisionTreeClassifier with SelectKMeans and k = 19 yields an F1 score of 0.700. I am very pleased with that result and have decided on the 19 features that I will use with the DecisionTreeClassifier. Any further improvement from this algorithm will come in the parameter tuning section of the investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar procedure will be carried out to determine the optimal number of features to use with the AdaBoostClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=10, score_func=<function f_classif at 0x000000000A01DD68>)), ('classify', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'select_features__k': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = np.arange(1, len(features_list))\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "ada_clf= GridSearchCV(pipe, param_grid=param_grid, scoring='f1', cv =10)\n",
    "ada_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 23}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65952380952380951"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Features F-statistics:\n",
      "\n",
      "bonus : 36.2022\n",
      "from_poi_ratio : 25.8246\n",
      "salary : 25.4609\n",
      "total_stock_value : 24.8372\n",
      "exercised_stock_options : 24.0888\n",
      "bonus_to_total : 20.8908\n",
      "deferred_income : 17.0963\n",
      "bonus_to_salary : 17.0251\n",
      "shared_poi_ratio : 16.1635\n",
      "shared_receipt_with_poi : 14.7385\n",
      "from_poi_to_this_person : 12.9213\n",
      "long_term_incentive : 12.7397\n",
      "total_payments : 10.2104\n",
      "restricted_stock : 10.0832\n",
      "other : 7.9157\n",
      "loan_advances : 7.0635\n",
      "expenses : 5.7008\n",
      "from_this_person_to_poi : 3.4099\n",
      "to_poi_ratio : 2.8564\n",
      "director_fees : 2.1897\n",
      "to_messages : 1.0279\n",
      "from_messages : 0.9875\n",
      "restricted_stock_deferred : 0.7876\n"
     ]
    }
   ],
   "source": [
    "ada_selection = SelectKBest(k=23)\n",
    "ada_selection.fit_transform(features, labels)\n",
    "\n",
    "ada_scores = ada_selection.scores_\n",
    "ada_features = zip(ada_scores, features_list[1:])\n",
    "ada_features = sorted(ada_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "print('AdaBoost Features F-statistics:\\n')\n",
    "for i in range(23):\n",
    "    print('{} : {:.4f}'.format(ada_features[i][1], ada_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.92857\tPrecision: 0.77778\tRecall: 0.70000\tF1: 0.73684\tF2: 0.71429\n",
      "\tTotal predictions:  140\tTrue positives:   14\tFalse positives:    4\tFalse negatives:    6\tTrue negatives:  116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.dump_classifier_and_data(ada_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal number of parameters for SelectKMeans using GridSearch for the AdaBoostClassifier was 23. This resulted in a slightly lower F1 score of 0.689, but still quite high. I will use the 23 highest scoring features with the AdaBoostClassifier. The F-Scores are the same so I will not show them all again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I could also perform Principal Component Analysis, but I think that the performance I am seeing is already high and the algorithms do not take very long to train even on a large number of features. PCA creates new features that do not necessarily represent actual quantifiable values in the dataset, and I like the idea that I know exactly all the features I am putting into the model. This is one way that I try to combat the [black box problem](http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731) in machine learning. If I at least know what is going in, then I can try to understand why the model returned a certain classification and maybe it can inform my thinking and help me to create smarter machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Algorithm Tuning </h1>\n",
    "\n",
    "The next step is to begin tuning the classifiers. I will use GridSearchCV again and I will input a wide variety of parameters to try in the parameter grid. The decision tree classifier will be up first. Looking at the sci-kit learn [documentation for the DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), there are a number of parameters that can be tuned and I will focus on four. The first will be __criterion__ for splitting, either 'gini', or 'entropy' to maximize the [information gain](https://en.wikipedia.org/wiki/Information_gain_ratio). The other three I will try will be __min_samples_split__, __max_depth__, and __max_features__. I will continue to use a cross-validation with 10 splits in the grid search and the scoring criterion will remain set at F1 because that is what I am trying to maximize.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x000000000A01DD68>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'classify__max_features': [None, 'sqrt', 'log2', 'auto'], 'classify__min_samples_split': [2, 4, 6, 8, 10, 20], 'classify__criterion': ['gini', 'entropy'], 'classify__max_depth': [None, 5, 10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_pipe = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = dict(classify__criterion = ['gini', 'entropy'] , \n",
    "                  classify__min_samples_split = [2, 4, 6, 8, 10, 20],\n",
    "                  classify__max_depth = [None, 5, 10, 15, 20],\n",
    "                  classify__max_features = [None, 'sqrt', 'log2', 'auto'])\n",
    "\n",
    "tree_clf = GridSearchCV(tree_pipe, param_grid = param_grid, scoring='f1', cv=10)\n",
    "tree_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82190476190476203"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify__criterion': 'entropy',\n",
       " 'classify__max_depth': None,\n",
       " 'classify__max_features': None,\n",
       " 'classify__min_samples_split': 20}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now implement the best parameters selected by the Grid Search and cross validation them using the tester.py function. I am seeing quite high F1 scores, and I am wondering if perhaps I am making some large mistake, such as training on the same data that I test on. I don't think this is so because I am using cross validation in the GridSearch, but it only takes a few seconds to test so I might as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.97143\tPrecision: 0.86364\tRecall: 0.95000\tF1: 0.90476\tF2: 0.93137\n",
      "\tTotal predictions:  140\tTrue positives:   19\tFalse positives:    3\tFalse negatives:    1\tTrue negatives:  117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=20)),\n",
    "    ('classify', DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features=None, min_samples_split=20))\n",
    "])\n",
    "\n",
    "tester.dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the cross validation in tester.py, my F1 score is indeed very high with the recommended parameters. Now, I am satisfied with the recall and precision score of the DecisionTreeClassifier. I will play around with the AdaBoostClassifier using the same approach because I am  curious to see if I can beat the F1 score. \n",
    "\n",
    "It's time to look at the Sci-kit learn [documentation for the AdaBoost Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) to see the parameters available to tune. The AdaBoostClassifier boosts another 'base' classifier, which by default is the Decision Tree. I can alter this using the __base_estimator__ parameter to tree out a random forest and the gaussian naive bayes classification. Ideally, AdaBoost is used on weak classifiers, or those that perform only slightly better than random. One example would be a [decision stump](http://stackoverflow.com/questions/12097155/weak-classifier) or a decision tree with only a single layer. However, I will try stronger classifiers for the base as well. The other parameters I can change are __n_estimators__ which is how many weak models to fit and __learning_rate__, a measure of the weight given to each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_pipe = Pipeline([('select_features', SelectKBest(k=20)),\n",
    "                     ('classify', AdaBoostClassifier())\n",
    "                    ])\n",
    "\n",
    "param_grid = dict(classify__base_estimator=[DecisionTreeClassifier(), RandomForestClassifier(), GaussianNB()],\n",
    "                  classify__n_estimators = [30, 50, 70, 120],\n",
    "                  classify__learning_rate = [0.5, 1, 1.5, 2, 4])\n",
    "\n",
    "ada_clf = GridSearchCV(ada_pipe, param_grid=param_grid, scoring='f1', cv=10)\n",
    "ada_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here is a brief summary of the results using GridSearch for feature selection and then for algorithm tuning.\n",
    "\n",
    "__Decision Tree Classifier__:\n",
    "\n",
    "| Parameter         | SelectKBest | criterion | max_depth | max_features | min\\_samples\\_split |\n",
    "|------------------ |-------------|-----------|-----------|--------------|---------------------|\n",
    "| __Optimal Value__ |      19     | 'entropy' |    None   |     None     |          8          |\n",
    "\n",
    "__AdaBoost Classifier__:\n",
    "\n",
    "| Parameter         | SelectKBest | base\\_estimator        | learning\\_rate | n\\_estimators |\n",
    "|-------------------|-------------|------------------------|----------------|---------------|\n",
    "| __Optimal Value__ |      20     | DecisionTreeClassifier |       1.5      |      120      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to create my model with the optimal parameters. I will test both classifiers using the provided tester function one more time. This function is the official scoring benchmark and utilizes cross-validation, so it will serve as the judge of the quality of the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier(criterion='entropy', max_depth=None, max_features=None, min_samples_split=20))\n",
    "])\n",
    "\n",
    "dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_clf = Pipeline([('select_features', SelectKBest(k=20)),\n",
    "                   ('classify', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), learning_rate=1.5, n_estimators=120))\n",
    "                   ])\n",
    "\n",
    "dump_classifier_and_data(ada_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Conclusions <\\h1>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
