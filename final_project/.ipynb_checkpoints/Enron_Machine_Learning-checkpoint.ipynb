{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load in the email and financial data with all of the features. I will convert the dictionary into a pandas dataframe for easier cleaning and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tester\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". \n",
    "payment_data = ['salary',\n",
    "                'bonus',\n",
    "                'long_term_incentive',\n",
    "                'deferred_income',\n",
    "                'deferral_payments',\n",
    "                'loan_advances',\n",
    "                'other',\n",
    "                'expenses',                \n",
    "                'director_fees', \n",
    "                'total_payments']\n",
    "\n",
    "stock_data = ['exercised_stock_options',\n",
    "              'restricted_stock',\n",
    "              'restricted_stock_deferred',\n",
    "              'total_stock_value']\n",
    "\n",
    "email_data = ['to_messages',\n",
    "              'from_messages',\n",
    "              'from_poi_to_this_person',\n",
    "              'from_this_person_to_poi',\n",
    "              'shared_receipt_with_poi']\n",
    "              \n",
    "              \n",
    "features_list = ['poi'] + payment_data + stock_data + email_data\n",
    "                 # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = df.replace('NaN', np.nan)\n",
    "df = df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 20 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "dtypes: bool(1), float64(19)\n",
      "memory usage: 23.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to convert all of the data types to floating point numbers except for the poi column which can remain as a boolean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the official documentation for the dataset, values of NaN in the financial dataset represent 0 and not unknown quantities. However, for the email data, NaNs stand for unknown information. Therefore, I will replace any financial data that is NaN with 0 but will fill in the NaNs for the email data with the median of the column grouped by person of interest. In other words, if a person has a NaN value for 'to_messages', and they are a person of interest, I will fill in that value with the median value of 'to_messages' for a person of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[payment_data] = df[payment_data].fillna(0)\n",
    "df[stock_data] = df[stock_data].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wkoehrse\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\n",
    "\n",
    "df_poi = df[df['poi'] == True]\n",
    "df_nonpoi = df[df['poi']==False]\n",
    "\n",
    "df_poi.ix[:, email_data] = imp.fit_transform(df_poi.ix[:,email_data])\n",
    "df_nonpoi.ix[:, email_data] = imp.fit_transform(df_nonpoi.ix[:,email_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_poi.append(df_nonpoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to check for outliers/incorrect data is to add up all of the payment related columns for each person and see if that is equal to the total payment recorded for the individual. I can also do the same for stock payments. If the data was entered by hand, I would expect that there would be at least a few errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "      <td>2007.111111</td>\n",
       "      <td>668.763889</td>\n",
       "      <td>58.5</td>\n",
       "      <td>36.277778</td>\n",
       "      <td>1058.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>463.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False     0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY  False     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT              -44093.0  2007.111111     668.763889   \n",
       "BHATNAGAR SANJAY                0.0   523.000000      29.000000   \n",
       "\n",
       "                  from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "BELFER ROBERT                        58.5                36.277778   \n",
       "BHATNAGAR SANJAY                      0.0                 1.000000   \n",
       "\n",
       "                  shared_receipt_with_poi  \n",
       "BELFER ROBERT                 1058.527778  \n",
       "BHATNAGAR SANJAY               463.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[payment_data[:-1]].sum(axis='columns') != df['total_payments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correct the discrepancies which most likely arise from incorrect data entry, I can use the official financial data gathered by FineLaw and [available through Udacity's GitHub](https://github.com/udacity/ud120-projects/blob/master/final_project/enron61702insiderpay.pdf). \n",
    "For Robert Belfer, the financial data has been shifted one column to the right, and for Sanjay Bhatnagar, the financial data has been shifted one column to the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the incorrect data for Belfer\n",
    "belfer_financial = df.ix['BELFER ROBERT', 1:15].tolist()\n",
    "# Delete the first element to shift left and add on a 0 to end as indicated in financial data\n",
    "belfer_financial.pop(0)\n",
    "belfer_financial.append(0)\n",
    "# Reinsert corrected data\n",
    "df.ix['BELFER ROBERT', 1:15] = belfer_financial\n",
    "\n",
    "# Retrieve the incorrect data for Bhatnagar\n",
    "bhatnagar_financial = df.ix['BHATNAGAR SANJAY', 1:15].tolist()\n",
    "# Delete the last element to shift right and add on a 0 to beginning\n",
    "bhatnagar_financial.pop(-1)\n",
    "bhatnagar_financial = [0] + bhatnagar_financial\n",
    "# Reinsert corrected data\n",
    "df.ix['BHATNAGAR SANJAY', 1:15] = bhatnagar_financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[payment_data[:-1]].sum(axis='columns') != df['total_payments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[stock_data[:-1]].sum(axis='columns') != df['total_stock_value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting the shifted financial data eliminated two errors. However, there may still be outliers in the dataset that need to be removed. Looking through the official financial PDF, I can see that I need to remove 'TOTAL' as it is entered as an individual (even though this is correct data, it is not a person and will be of no value when trying to identify persons of interest). Likewise, there is an entry for 'THE TRAVEL AGENCY IN THE PARK', which according to the documentation was a company co-owned by Enron's former Chairman's sister and is clearly not an individual that should be included in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=['TOTAL','THE TRAVEL AGENCY IN THE PARK'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now look for individual outliers. However, I will need to be conservative in terms of removing the outliers because the dataset is rather small for machine learning in the first place. Moreover, the outliers might actually be important as they could represent patterns in the data that would aid in the identification of persons of interest. Using the [official definition of a mild outlier](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm) as either -1.5 times the Interquartile Range (IQR) below the 1st interquartile or +1.5 times the IQR above the 3rd quartile, I will count the number of columns in which each individual is an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IQR = df.quantile(q=0.75) - df.quantile(q=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quartile = df.quantile(q=0.25)\n",
    "third_quartile = df.quantile(q=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAY KENNETH L         15\n",
       "FREVERT MARK A        12\n",
       "BELDEN TIMOTHY N       9\n",
       "SKILLING JEFFREY K     9\n",
       "BAXTER JOHN C          8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = df[(df>(third_quartile + 1.5*IQR) ) | (df<(first_quartile - 1.5*IQR) )].count(axis=1)\n",
    "outliers.sort_values(axis=0, ascending=False, inplace=True)\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this point, I need to do some research before blinding deleting outliers, especially if the outliers are persons of interest. Based on the small number of persons of interest initially in the dataset, I will choose to not remove any individuals who are persons are interest regardless of the number of outliers they may have. An outlier could be a sign of fradulent activity, as it could be evidence that someone is laundering illegal funds through the company payroll or maybe an accomplish is being paid to remain quiet about the activity. I will examine the top seven outliers which is around 5% of the total dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = outliers[:7].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAY KENNETH L',\n",
       " 'FREVERT MARK A',\n",
       " 'BELDEN TIMOTHY N',\n",
       " 'SKILLING JEFFREY K',\n",
       " 'BAXTER JOHN C',\n",
       " 'LAVORATO JOHN J',\n",
       " 'DELAINEY DAVID W']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = df.ix[outliers, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>True</td>\n",
       "      <td>1072321.0</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>3600000.0</td>\n",
       "      <td>-300000.0</td>\n",
       "      <td>202911.0</td>\n",
       "      <td>81525000.0</td>\n",
       "      <td>10359729.0</td>\n",
       "      <td>99832.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103559793.0</td>\n",
       "      <td>34348384.0</td>\n",
       "      <td>14761694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49110078.0</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>False</td>\n",
       "      <td>1060932.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>1617011.0</td>\n",
       "      <td>-3367011.0</td>\n",
       "      <td>6426990.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>7427621.0</td>\n",
       "      <td>86987.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17252530.0</td>\n",
       "      <td>10433518.0</td>\n",
       "      <td>4188667.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14622185.0</td>\n",
       "      <td>3275.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>242.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2979.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BELDEN TIMOTHY N</th>\n",
       "      <td>True</td>\n",
       "      <td>213999.0</td>\n",
       "      <td>5249999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2334434.0</td>\n",
       "      <td>2144013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210698.0</td>\n",
       "      <td>17355.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5501630.0</td>\n",
       "      <td>953136.0</td>\n",
       "      <td>157569.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1110705.0</td>\n",
       "      <td>7991.000000</td>\n",
       "      <td>484.000000</td>\n",
       "      <td>228.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>5521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>True</td>\n",
       "      <td>1111258.0</td>\n",
       "      <td>5600000.0</td>\n",
       "      <td>1920000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22122.0</td>\n",
       "      <td>29336.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8682716.0</td>\n",
       "      <td>19250000.0</td>\n",
       "      <td>6843672.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26093672.0</td>\n",
       "      <td>3627.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2042.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>False</td>\n",
       "      <td>267102.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1586055.0</td>\n",
       "      <td>-1386055.0</td>\n",
       "      <td>1295738.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2660303.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5634343.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>3942714.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10623258.0</td>\n",
       "      <td>2007.111111</td>\n",
       "      <td>668.763889</td>\n",
       "      <td>58.5</td>\n",
       "      <td>36.277778</td>\n",
       "      <td>1058.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAVORATO JOHN J</th>\n",
       "      <td>False</td>\n",
       "      <td>339288.0</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>2035380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>49537.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10425757.0</td>\n",
       "      <td>4158995.0</td>\n",
       "      <td>1008149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5167144.0</td>\n",
       "      <td>7259.000000</td>\n",
       "      <td>2585.000000</td>\n",
       "      <td>528.0</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>3962.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELAINEY DAVID W</th>\n",
       "      <td>True</td>\n",
       "      <td>365163.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>1294981.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>86174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4747979.0</td>\n",
       "      <td>2291113.0</td>\n",
       "      <td>1323148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3614261.0</td>\n",
       "      <td>3093.000000</td>\n",
       "      <td>3069.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>2097.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      poi     salary      bonus  long_term_incentive  \\\n",
       "LAY KENNETH L        True  1072321.0  7000000.0            3600000.0   \n",
       "FREVERT MARK A      False  1060932.0  2000000.0            1617011.0   \n",
       "BELDEN TIMOTHY N     True   213999.0  5249999.0                  0.0   \n",
       "SKILLING JEFFREY K   True  1111258.0  5600000.0            1920000.0   \n",
       "BAXTER JOHN C       False   267102.0  1200000.0            1586055.0   \n",
       "LAVORATO JOHN J     False   339288.0  8000000.0            2035380.0   \n",
       "DELAINEY DAVID W     True   365163.0  3000000.0            1294981.0   \n",
       "\n",
       "                    deferred_income  deferral_payments  loan_advances  \\\n",
       "LAY KENNETH L             -300000.0           202911.0     81525000.0   \n",
       "FREVERT MARK A           -3367011.0          6426990.0      2000000.0   \n",
       "BELDEN TIMOTHY N         -2334434.0          2144013.0            0.0   \n",
       "SKILLING JEFFREY K              0.0                0.0            0.0   \n",
       "BAXTER JOHN C            -1386055.0          1295738.0            0.0   \n",
       "LAVORATO JOHN J                 0.0                0.0            0.0   \n",
       "DELAINEY DAVID W                0.0                0.0            0.0   \n",
       "\n",
       "                         other  expenses  director_fees  total_payments  \\\n",
       "LAY KENNETH L       10359729.0   99832.0            0.0     103559793.0   \n",
       "FREVERT MARK A       7427621.0   86987.0            0.0      17252530.0   \n",
       "BELDEN TIMOTHY N      210698.0   17355.0            0.0       5501630.0   \n",
       "SKILLING JEFFREY K     22122.0   29336.0            0.0       8682716.0   \n",
       "BAXTER JOHN C        2660303.0   11200.0            0.0       5634343.0   \n",
       "LAVORATO JOHN J         1552.0   49537.0            0.0      10425757.0   \n",
       "DELAINEY DAVID W        1661.0   86174.0            0.0       4747979.0   \n",
       "\n",
       "                    exercised_stock_options  restricted_stock  \\\n",
       "LAY KENNETH L                    34348384.0        14761694.0   \n",
       "FREVERT MARK A                   10433518.0         4188667.0   \n",
       "BELDEN TIMOTHY N                   953136.0          157569.0   \n",
       "SKILLING JEFFREY K               19250000.0         6843672.0   \n",
       "BAXTER JOHN C                     6680544.0         3942714.0   \n",
       "LAVORATO JOHN J                   4158995.0         1008149.0   \n",
       "DELAINEY DAVID W                  2291113.0         1323148.0   \n",
       "\n",
       "                    restricted_stock_deferred  total_stock_value  to_messages  \\\n",
       "LAY KENNETH L                             0.0         49110078.0  4273.000000   \n",
       "FREVERT MARK A                            0.0         14622185.0  3275.000000   \n",
       "BELDEN TIMOTHY N                          0.0          1110705.0  7991.000000   \n",
       "SKILLING JEFFREY K                        0.0         26093672.0  3627.000000   \n",
       "BAXTER JOHN C                             0.0         10623258.0  2007.111111   \n",
       "LAVORATO JOHN J                           0.0          5167144.0  7259.000000   \n",
       "DELAINEY DAVID W                          0.0          3614261.0  3093.000000   \n",
       "\n",
       "                    from_messages  from_poi_to_this_person  \\\n",
       "LAY KENNETH L           36.000000                    123.0   \n",
       "FREVERT MARK A          21.000000                    242.0   \n",
       "BELDEN TIMOTHY N       484.000000                    228.0   \n",
       "SKILLING JEFFREY K     108.000000                     88.0   \n",
       "BAXTER JOHN C          668.763889                     58.5   \n",
       "LAVORATO JOHN J       2585.000000                    528.0   \n",
       "DELAINEY DAVID W      3069.000000                     66.0   \n",
       "\n",
       "                    from_this_person_to_poi  shared_receipt_with_poi  \n",
       "LAY KENNETH L                     16.000000              2411.000000  \n",
       "FREVERT MARK A                     6.000000              2979.000000  \n",
       "BELDEN TIMOTHY N                 108.000000              5521.000000  \n",
       "SKILLING JEFFREY K                30.000000              2042.000000  \n",
       "BAXTER JOHN C                     36.277778              1058.527778  \n",
       "LAVORATO JOHN J                  411.000000              3962.000000  \n",
       "DELAINEY DAVID W                 609.000000              2097.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few considerations to make here:\n",
    "1. Kenneth Lay, [the CEO of Enron from 1986-2001](http://www.biography.com/people/kenneth-lay-234611), presided over many of the illegal business activites and hence is one of the most important persons of interest. \n",
    "2. Mark Frevert served as chief executive of [Enron Europe from 1986-2000 and was appointed as chairman of Enron in 2001](http://www.risk.net/risk-management/2123422/ten-years-after-its-collapse-enron-lives-energy-markets). He was a major player in the firm, although not a person of interest. I believe that he is not representative of the average employee at Enron during this time because of his substantial compensation and will remove him from the dataset. \n",
    "3. Timothy Belden was the [former head of trading for Enron](http://articles.latimes.com/2007/feb/15/business/fi-enron15) who developed the strategy to illegally raise energy prices in California. He was a person of interest and will definitely remain in the dataset. \n",
    "4. Jeffrey Skilling [replaced Kenneth Lay as CEO of Enron in 2001 and orchestrated much of the fraud](http://www.biography.com/people/jeffrey-skilling-235386) that destroyed Enron. As a person of interest, he will remain in the dataset. \n",
    "5. John Baxter was a former vice Enron vice chairman and [died of an apparent self-inflicted gunshot](https://www.wsws.org/en/articles/2002/01/enro-j28.html) before he was able to testify against other Enron executives. I will remove him from the dataset as he is not a person of interest. \n",
    "6. John Lavorato was a top executive in the energy-trading branch of Enron and received large bonuses to [keep him from leaving Enron](http://www.nytimes.com/2002/06/18/business/officials-got-a-windfall-before-enron-s-collapse.html). As he was not a person of interest, and the large bonus ended up skewing his total pay towards the top of the range, I think it would be appropriate to remove him from the dataset. \n",
    "4. Lawrence Whalley [served as the president of Enron](http://www.corpwatch.org/article.php?id=13194) and fired Andrew Fastow once it was apparent the severity of Enron's situation. He was investigated thoroughly but not identified as a person of interest and therefore will be removed from the dataset.  \n",
    "\n",
    "Total, that is four people to remove from the dataset. I believe these removals are justified primarily because none of these individuals were persons of interest and they all were upper-level executives with pay levels far above the average employee. I hesitate to remove any samples from the data, but I believe that removing these individuals will improve the quality of the classifier. I can try with and without removing these individuals and measure the accuracy, precision, and recall of the classifier to determine if my choice was justified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=['FREVERT MARK A', 'LAVORATO JOHN J', 'WHALLEY LAWRENCE G', 'BAXTER JOHN C'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    122\n",
       "True      18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df==0].count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 2800 observations of financial and email data in the set now that the data cleaning has been finished. Of these, __1150 or 41%__ are 0 financial values. There are 18 persons of interest, comprising __12.9%__ of the individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to begin training some classifiers with default parameters in order to identify existing features that are most predicative of persons of interest. After that, if the classifier performance is low, I will try and devise additional features and then fine-tune the algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Algorithm Testing\n",
    "\n",
    "The four algorithms I have selected for initial testing are Gaussian Naive Bayes (GaussianNB), DecisionTreeClassifier, Support Vector Classifier (SVC), and KMeans. I will run all of the algorithms with the default parameters except I will alter the kernel used in the Support Vector Machine to be linear. I will also select 2 to be the number of clusters for KMeans as I know in advance that there are two categories I want to identify. Although accuracy would seem to be the obvious choice for evaluating the quality of a classifier, accuracy can be a crude measure at times and is not suited for some datasets including this one. For example, if a classifier were to guess that all of the samples in my cleaned dataset were _not_ persons of interest, it would have an accuracy of 87.1%. However, this clearly would not satisfy the objective of this investigation which is to create a classifier that can identify persons of interest. Therefore, different metrics are needed to evaluate the tuned algorithm (a classifier is the algorithm plus the parameters selected) to gauge its effectiveness. The two selected for this project are [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "* __Precision__ is the number of correct positive results over the total number of positive labels assigned. In other words, it is the fraction of persons of interest predicted by the algorithm that are truly persons of interest.  Mathematically precision is defined as \n",
    "\n",
    "$$ precision = \\frac{true\\ positives}{true\\ positives + false\\ positives} $$ \n",
    "\n",
    "* __Recall__ is the number of correct positive results divided by the number of positive results that should have been identified. In other words, it is the fraction of the total number of persons of interest that the classifier correctly labels. Mathematically, recall is defined as\n",
    "\n",
    "$$ recall = \\frac{true\\ positives}{true\\ positives + false\\ negatives} $$ \n",
    "\n",
    "Precision is also known as positive predictive value while recall is the sensitivity of the classifier. A combined measured of precision and recall is the [__F1 score__](https://en.wikipedia.org/wiki/F1_score). Is it the harmonic mean of precision and recall. Mathematically, the F1 score is defined as:\n",
    "\n",
    "$$ F1\\ Score = \\frac{2\\ (precision\\ x\\ recall)}{precision + recall} $$\n",
    "\n",
    "For this project, the objective was a precision and a recall greater than 0.3. However, I believe it is possible to do much better than that with the right feature selection and algorithm tuning. \n",
    "\n",
    "The only preparation I will do for this initial testing of the different algorithms is to scale the data such that it has a zero mean and a unit variance. This process is called [normalization](http://www.analytictech.com/ba762/handouts/normalization.htm) and is accomplished using the scale function from the sklearn preprocessing module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.70714\tPrecision: 0.30909\tRecall: 0.85000\tF1: 0.45333\tF2: 0.62963\n",
      "\tTotal predictions:  140\tTrue positives:   17\tFalse positives:   38\tFalse negatives:    3\tTrue negatives:   82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import tester\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "scaled_df = df\n",
    "scaled_df.ix[:,1:] = scale(df.ix[:,1:])\n",
    "data_dict = scaled_df.to_dict(orient='index')\n",
    "\n",
    "my_dataset = data_dict\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.84286\tPrecision: 0.45000\tRecall: 0.45000\tF1: 0.45000\tF2: 0.45000\n",
      "\tTotal predictions:  140\tTrue positives:    9\tFalse positives:   11\tFalse negatives:   11\tTrue negatives:  109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.86429\tPrecision: 0.57143\tRecall: 0.20000\tF1: 0.29630\tF2: 0.22989\n",
      "\tTotal predictions:  140\tTrue positives:    4\tFalse positives:    3\tFalse negatives:   16\tTrue negatives:  117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "\tAccuracy: 0.65000\tPrecision: 0.16279\tRecall: 0.35000\tF1: 0.22222\tF2: 0.28455\n",
      "\tTotal predictions:  140\tTrue positives:    7\tFalse positives:   36\tFalse negatives:   13\tTrue negatives:   84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from running the four classifiers on the entire featureset with no algorithm tuning are summarized in the table below\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.30909   | 0.85000 | 0.45333  | 0.70714  |\n",
    "| DecisionTree          | 0.5000    | 0.45000 | 0.47368  | 0.85714  |\n",
    "| SVC (kernel='linear') | 0.57143   | 0.2000  | 0.29630  | 0.86429  |\n",
    "| KMeans (n_clusters=2) | 0.17647   | 0.15000 | 0.16216  | 0.77857  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first run through the four algorithms, I can see that the decision tree performed best, followed by the gaussian naive bayes, support vector machine, and Kmeans clustering. In fact, the decision tree and naive Bayes classifiers both perform well enough to meet the standards for the project. Nonetheless, there is much work that can be done to improve these metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features\n",
    "\n",
    "The standard features for the dataset perform adequately, but not stellar. I want to define new features that will allow for more accurate predictions of poi. Additionally, I want to perform feature reduction using PCA in order to choose the dimensions that have the greatest variance and hopefully, predictive power. \n",
    "\n",
    "The first features I want to define are the number of emails to and from persons of interest scaled by the total number of emails sent by and received by an individual respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['to_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['from_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list.append('to_poi_ratio')\n",
    "features_list.append('from_poi_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'bonus',\n",
       " 'long_term_incentive',\n",
       " 'deferred_income',\n",
       " 'deferral_payments',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'expenses',\n",
       " 'director_fees',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'to_messages',\n",
       " 'from_messages',\n",
       " 'from_poi_to_this_person',\n",
       " 'from_this_person_to_poi',\n",
       " 'shared_receipt_with_poi',\n",
       " 'to_poi_ratio',\n",
       " 'from_poi_ratio']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_classif, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.76429\tPrecision: 0.35556\tRecall: 0.80000\tF1: 0.49231\tF2: 0.64000\n",
      "\tTotal predictions:  140\tTrue positives:   16\tFalse positives:   29\tFalse negatives:    4\tTrue negatives:   91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df = df\n",
    "scaled_df.ix[:,1:] = scale(df.ix[:,1:])\n",
    "data_dict = scaled_df.to_dict(orient='index')\n",
    "\n",
    "my_dataset = data_dict\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.89286\tPrecision: 0.63158\tRecall: 0.60000\tF1: 0.61538\tF2: 0.60606\n",
      "\tTotal predictions:  140\tTrue positives:   12\tFalse positives:    7\tFalse negatives:    8\tTrue negatives:  113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.87857\tPrecision: 0.71429\tRecall: 0.25000\tF1: 0.37037\tF2: 0.28736\n",
      "\tTotal predictions:  140\tTrue positives:    5\tFalse positives:    2\tFalse negatives:   15\tTrue negatives:  118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "\tAccuracy: 0.79286\tPrecision: 0.20000\tRecall: 0.15000\tF1: 0.17143\tF2: 0.15789\n",
      "\tTotal predictions:  140\tTrue positives:    3\tFalse positives:   12\tFalse negatives:   17\tTrue negatives:  108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding in only two additional features, I retested the algorithms with all of the features. The results are summarized in the table below.\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.35556   | 0.80000 | 0.49321  | 0.76429  |\n",
    "| DecisionTree          | 0.60000   | 0.60000 | 0.60000  | 0.88571  |\n",
    "| SVC (kernel='linear') | 0.71429 | 0.25000 | 0.37037  | 0.87857  |\n",
    "| KMeans (n_clusters=2) | 0.12500   | 0.25000 | 0.16667  | 0.64286  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 Score improved for all four of the classifiers with the addition of the two additional created features. The F1 score for the decision tree is stil the highest followed by the Gaussian Naive Bayes. At this point, neither the SVC with the linear kernel nor the KMeans clustering pass the standards of 0.3 for precision and recall. I will drop the latter two algorithms and I will also drop the GaussianNB in favor of [AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf) because it has GaussianNB does not have any tunable parameters and therefore I will not be able to improve the precision or recall any beyond what I do to alter the features. AdaBoost fits multiple classifiers on a dataset and adjusts the weights of incorrectly classified instances with each iteration to concentrate on the difficult to classify samples. \n",
    "\n",
    "The next step I want to take is feature selection. Looking at the features importances for both AdaBoost and DecisionTree, I can see that there are some features that have a zero importance. I will use selectKbest to select the best performing features and use gridSearchCV to pick the optimum number of features to select. I will also add one more feature, shared_ratio, or the ratio of messags an indidual recieved that were shared with a person of interest divided by the total number of emails received by that individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['shared_poi_ratio'] = df['shared_receipt_with_poi'] / df['to_messages']\n",
    "\n",
    "features_list.append('shared_poi_ratio')\n",
    "\n",
    "scaled_df = df\n",
    "scaled_df.ix[:,1:] = scale(df.ix[:,1:])\n",
    "data_dict = scaled_df.to_dict(orient='index')\n",
    "\n",
    "my_dataset = data_dict\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'bonus',\n",
       " 'long_term_incentive',\n",
       " 'deferred_income',\n",
       " 'deferral_payments',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'expenses',\n",
       " 'director_fees',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'to_messages',\n",
       " 'from_messages',\n",
       " 'from_poi_to_this_person',\n",
       " 'from_this_person_to_poi',\n",
       " 'shared_receipt_with_poi',\n",
       " 'to_poi_ratio',\n",
       " 'from_poi_ratio',\n",
       " 'shared_poi_ratio']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.87143\tPrecision: 0.55556\tRecall: 0.50000\tF1: 0.52632\tF2: 0.51020\n",
      "\tTotal predictions:  140\tTrue positives:   10\tFalse positives:    8\tFalse negatives:   10\tTrue negatives:  112\n",
      "\n",
      "to_poi_ratio : 0.2841\n",
      "from_poi_ratio : 0.1954\n",
      "salary : 0.1596\n",
      "bonus : 0.1534\n",
      "restricted_stock_deferred : 0.0614\n",
      "poi : 0.0573\n",
      "total_stock_value : 0.0477\n",
      "loan_advances : 0.0206\n",
      "other : 0.0123\n",
      "from_poi_to_this_person : 0.0082\n"
     ]
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier()\n",
    "dump_classifier_and_data(clf_tree, my_dataset, features_list)\n",
    "clf_tree = tester.main()\n",
    "\n",
    "tree_feature_importances = [0] + (clf_tree.feature_importances_)\n",
    "tree_features = zip(tree_feature_importances, features_list)\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.90000\tPrecision: 0.66667\tRecall: 0.60000\tF1: 0.63158\tF2: 0.61224\n",
      "\tTotal predictions:  140\tTrue positives:   12\tFalse positives:    6\tFalse negatives:    8\tTrue negatives:  114\n",
      "\n",
      "salary : 0.1400\n",
      "loan_advances : 0.1000\n",
      "to_messages : 0.1000\n",
      "to_poi_ratio : 0.1000\n",
      "long_term_incentive : 0.0800\n",
      "other : 0.0800\n",
      "shared_receipt_with_poi : 0.0800\n",
      "restricted_stock_deferred : 0.0600\n",
      "from_messages : 0.0600\n",
      "from_poi_to_this_person : 0.0400\n"
     ]
    }
   ],
   "source": [
    "clf_ada = AdaBoostClassifier()\n",
    "dump_classifier_and_data(clf_ada, my_dataset, features_list)\n",
    "clf_ada = tester.main()\n",
    "ada_feature_importances = [0] + clf_ada.feature_importances_\n",
    "ada_features = zip(ada_feature_importances, features_list)\n",
    "for pair in ada_features:\n",
    "    importance = round(pair[0],4)\n",
    "    if importance == 0.0:\n",
    "        ada_features.remove(pair)\n",
    "        \n",
    "ada_features = sorted(ada_features, key=lambda x:x[0], reverse=True)\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(ada_features[i][1], ada_features[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is interesting to compare the feature importances for the DecisionTree and the AdaBoost classifiers. They definitely are not in close agreement for the top ten even though both manage a respectable F1 Score greater than 0.5. The next step is to make a pipeline and then let GridSearchCV do the tough work of selecting the optimal number of features to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cv = StratifiedShuffleSplit(random_state = 42)\n",
    "for train_idx, test_idx in cv.split(features, labels): \n",
    "    features_train = []\n",
    "    features_test  = []\n",
    "    labels_train   = []\n",
    "    labels_test    = []\n",
    "    for ii in train_idx:\n",
    "        features_train.append( features[ii] )\n",
    "        labels_train.append( labels[ii] )\n",
    "    for jj in test_idx:\n",
    "        features_test.append( features[jj] )\n",
    "        labels_test.append( labels[jj] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wkoehrse\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      1.00      0.96        12\n",
      "        1.0       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.93      0.93      0.92        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_features = np.arange(1, len(features_list))\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "tree_clf= GridSearchCV(pipe, param_grid=param_grid, scoring='f1')\n",
    "tree_clf.fit(features_train, labels_train)\n",
    "predictions = tree_clf.predict(features_test)\n",
    "print(classification_report(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50683421516754856"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=10, score_func=<function f_classif at 0x000000000BB206D8>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))]),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'select_features__k': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22])}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='f1', verbose=0)\n",
      "\tAccuracy: 0.87143\tPrecision: 0.56250\tRecall: 0.45000\tF1: 0.50000\tF2: 0.46875\n",
      "\tTotal predictions:  140\tTrue positives:    9\tFalse positives:    7\tFalse negatives:   11\tTrue negatives:  113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tree_clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.92143\tPrecision: 0.73684\tRecall: 0.70000\tF1: 0.71795\tF2: 0.70707\n",
      "\tTotal predictions:  140\tTrue positives:   14\tFalse positives:    5\tFalse negatives:    6\tTrue negatives:  115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 22}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=10, score_func=<function f_classif at 0x000000000BB206D8>)), ('classify', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'select_features__k': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = np.arange(1, len(features_list))\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "ada_clf= GridSearchCV(pipe, param_grid=param_grid, scoring='f1')\n",
    "ada_clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 19}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41131267321743514"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features__pca__n_components': 18, 'features__univ_select__k': 16}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pca = PCA()\n",
    "selection = SelectKBest()\n",
    "\n",
    "combined_features = FeatureUnion([('pca', pca), ('univ_select', selection)])\n",
    "\n",
    "features = combined_features.fit(features, labels).transform(features)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "pipeline = Pipeline([('features', combined_features), ('tree', tree)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=np.arange(1,22),\n",
    "                  features__univ_select__k=np.arange(1,22))\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring='f1')\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.545454545455\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features__pca__n_components': 1, 'features__univ_select__k': 18}\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "selection = SelectKBest()\n",
    "\n",
    "combined_features = FeatureUnion([('pca', pca), ('univ_select', selection)])\n",
    "\n",
    "features_train = combined_features.fit(features, labels)\n",
    "\n",
    "adaBoost = AdaBoostClassifier()\n",
    "\n",
    "pipeline = Pipeline([('features', combined_features), ('adaBoost', adaBoost)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=np.arange(1,22),\n",
    "                  features__univ_select__k=np.arange(1,22))\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring='f1')\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.613492063492\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I decided to run the grid search using both PCA and univariate feature selection with SelectKBest. I incorporated both of these into a single pipeline using the feature union method and the [documentation available on sci-kit learn](http://scikit-learn.org/stable/auto_examples/feature_stacker.html#sphx-glr-auto-examples-feature-stacker-py).  I used a cross-validation methodology of [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) which returns training and testing indexes based on a number of shuffling and splitting iterations. The scoring method I used for the GridSearch was F1 and therefore the search will return the parameters that maximum the F1 score. Running the GridSearch with a feature union for both the AdaBoost classifier and DecisionTreeClassifier returned the following optimal parameters and F1 Score:\n",
    "\n",
    "| Algorithm              | PCA: n_components | SelectKBest: k | F1 Score |\n",
    "|------------------------|-------------------|----------------|----------|\n",
    "| DecisionTreeClassifier |         18        |        7       | 0.63063  |\n",
    "| AdaBoostClassifier     |         1         |       18       | 0.61349  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point forward, I will be using these features to tune the algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pca = PCA(n_components=18)\n",
    "tree_selection = SelectKBest(k=16)\n",
    "\n",
    "tree_combined_features = FeatureUnion([('pca', tree_pca), ('univ_select', tree_selection)])\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree_pipeline = Pipeline([('features', tree_combined_features),('tree', tree)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=18, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=16, score_func=<function f_classif at 0x000000000BB206D8>))],\n",
      "       trans...it=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.82143\tPrecision: 0.38095\tRecall: 0.40000\tF1: 0.39024\tF2: 0.39604\n",
      "\tTotal predictions:  140\tTrue positives:    8\tFalse positives:   13\tFalse negatives:   12\tTrue negatives:  107\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=18, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=16, score_func=<function f_classif at 0x000000000BB206D8>))],\n",
       "       trans...it=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_classifier_and_data(tree_pipeline, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=18, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=16, score_func=<function f_classif at 0x000000000BB206D8>))],\n",
       "       trans...it=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))])>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_pipeline.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
