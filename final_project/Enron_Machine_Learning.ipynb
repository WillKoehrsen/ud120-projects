{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load in the email and financial data with all of the features. I will convert the dictionary into a pandas dataframe for easier cleaning and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tester\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". \n",
    "payment_data = ['salary',\n",
    "                'bonus',\n",
    "                'long_term_incentive',\n",
    "                'deferred_income',\n",
    "                'deferral_payments',\n",
    "                'loan_advances',\n",
    "                'other',\n",
    "                'expenses',                \n",
    "                'director_fees', \n",
    "                'total_payments']\n",
    "\n",
    "stock_data = ['exercised_stock_options',\n",
    "              'restricted_stock',\n",
    "              'restricted_stock_deferred',\n",
    "              'total_stock_value']\n",
    "\n",
    "email_data = ['to_messages',\n",
    "              'from_messages',\n",
    "              'from_poi_to_this_person',\n",
    "              'from_this_person_to_poi',\n",
    "              'shared_receipt_with_poi']\n",
    "              \n",
    "              \n",
    "features_list = ['poi'] + payment_data + stock_data + email_data\n",
    "                 # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = df.replace('NaN', np.nan)\n",
    "df = df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 20 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "dtypes: bool(1), float64(19)\n",
      "memory usage: 23.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to convert all of the data types to floating point numbers except for the poi column which can remain as a boolean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the official documentation for the dataset, values of NaN in the financial dataset represent 0 and not unknown quantities. However, for the email data, NaNs stand for unknown information. Therefore, I will replace any financial data that is NaN with 0 but will fill in the NaNs for the email data with the median of the column grouped by person of interest. In other words, if a person has a NaN value for 'to_messages', and they are a person of interest, I will fill in that value with the median value of 'to_messages' for a person of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[payment_data] = df[payment_data].fillna(0)\n",
    "df[stock_data] = df[stock_data].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will Koehrsen\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\n",
    "\n",
    "df_poi = df[df['poi'] == True]\n",
    "df_nonpoi = df[df['poi']==False]\n",
    "\n",
    "df_poi.ix[:, email_data] = imp.fit_transform(df_poi.ix[:,email_data])\n",
    "df_nonpoi.ix[:, email_data] = imp.fit_transform(df_nonpoi.ix[:,email_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_poi.append(df_nonpoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to check for outliers/incorrect data is to add up all of the payment related columns for each person and see if that is equal to the total payment recorded for the individual. I can also do the same for stock payments. If the data was entered by hand, I would expect that there would be at least a few errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "      <td>2007.111111</td>\n",
       "      <td>668.763889</td>\n",
       "      <td>58.5</td>\n",
       "      <td>36.277778</td>\n",
       "      <td>1058.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>463.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False     0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY  False     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT              -44093.0  2007.111111     668.763889   \n",
       "BHATNAGAR SANJAY                0.0   523.000000      29.000000   \n",
       "\n",
       "                  from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "BELFER ROBERT                        58.5                36.277778   \n",
       "BHATNAGAR SANJAY                      0.0                 1.000000   \n",
       "\n",
       "                  shared_receipt_with_poi  \n",
       "BELFER ROBERT                 1058.527778  \n",
       "BHATNAGAR SANJAY               463.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[payment_data[:-1]].sum(axis='columns') != df['total_payments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correct the discrepancies which most likely arise from incorrect data entry, I can use the official financial data gathered by FineLaw and [available through Udacity's GitHub](https://github.com/udacity/ud120-projects/blob/master/final_project/enron61702insiderpay.pdf). \n",
    "For Robert Belfer, the financial data has been shifted one column to the right, and for Sanjay Bhatnagar, the financial data has been shifted one column to the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the incorrect data for Belfer\n",
    "belfer_financial = df.ix['BELFER ROBERT', 1:15].tolist()\n",
    "# Delete the first element to shift left and add on a 0 to end as indicated in financial data\n",
    "belfer_financial.pop(0)\n",
    "belfer_financial.append(0)\n",
    "# Reinsert corrected data\n",
    "df.ix['BELFER ROBERT', 1:15] = belfer_financial\n",
    "\n",
    "# Retrieve the incorrect data for Bhatnagar\n",
    "bhatnagar_financial = df.ix['BHATNAGAR SANJAY', 1:15].tolist()\n",
    "# Delete the last element to shift right and add on a 0 to beginning\n",
    "bhatnagar_financial.pop(-1)\n",
    "bhatnagar_financial = [0] + bhatnagar_financial\n",
    "# Reinsert corrected data\n",
    "df.ix['BHATNAGAR SANJAY', 1:15] = bhatnagar_financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[payment_data[:-1]].sum(axis='columns') != df['total_payments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[stock_data[:-1]].sum(axis='columns') != df['total_stock_value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting the shifted financial data eliminated two errors. However, there may still be outliers in the dataset that need to be removed. Looking through the official financial PDF, I can see that I need to remove 'TOTAL' as it is entered as an individual (even though this is correct data, it is not a person and will be of no value when trying to identify persons of interest). Likewise, there is an entry for 'THE TRAVEL AGENCY IN THE PARK', which according to the documentation was a company co-owned by Enron's former Chairman's sister and is clearly not an individual that should be included in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=['TOTAL','THE TRAVEL AGENCY IN THE PARK'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now look for individual outliers. However, I will need to be conservative in terms of removing the outliers because the dataset is rather small for machine learning in the first place. Moreover, the outliers might actually be important as they could represent patterns in the data that would aid in the identification of persons of interest. Using the [official definition of a mild outlier](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm) as either -1.5 times the Interquartile Range (IQR) below the 1st interquartile or +1.5 times the IQR above the 3rd quartile, I will count the number of columns in which each individual is an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IQR = df.quantile(q=0.75) - df.quantile(q=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_quartile = df.quantile(q=0.25)\n",
    "third_quartile = df.quantile(q=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAY KENNETH L         15\n",
       "FREVERT MARK A        12\n",
       "BELDEN TIMOTHY N       9\n",
       "SKILLING JEFFREY K     9\n",
       "BAXTER JOHN C          8\n",
       "LAVORATO JOHN J        8\n",
       "DELAINEY DAVID W       7\n",
       "KEAN STEVEN J          7\n",
       "HAEDICKE MARK E        7\n",
       "WHALLEY LAWRENCE G     7\n",
       "RICE KENNETH D         6\n",
       "KITCHEN LOUISE         6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = df[(df>(third_quartile + 1.5*IQR) ) | (df<(first_quartile - 1.5*IQR) )].count(axis=1)\n",
    "outliers.sort_values(axis=0, ascending=False, inplace=True)\n",
    "outliers.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this point, I need to do some research before blinding deleting outliers, especially if the outliers are persons of interest. Based on the small number of persons of interest initially in the dataset, I will choose to not remove any individuals who are persons are interest regardless of the number of outliers they may have. An outlier could be a sign of fradulent activity, as it could be evidence that someone is laundering illegal funds through the company payroll or maybe an accomplish is being paid to remain quiet about the activity. I will examine the top seven outliers which is around 5% of the total dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliers = outliers[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAY KENNETH L',\n",
       " 'FREVERT MARK A',\n",
       " 'BELDEN TIMOTHY N',\n",
       " 'SKILLING JEFFREY K',\n",
       " 'BAXTER JOHN C',\n",
       " 'LAVORATO JOHN J',\n",
       " 'DELAINEY DAVID W',\n",
       " 'KEAN STEVEN J',\n",
       " 'HAEDICKE MARK E',\n",
       " 'WHALLEY LAWRENCE G']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_outliers = df.ix[outliers, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>True</td>\n",
       "      <td>1072321.0</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>3600000.0</td>\n",
       "      <td>-300000.0</td>\n",
       "      <td>202911.0</td>\n",
       "      <td>81525000.0</td>\n",
       "      <td>10359729.0</td>\n",
       "      <td>99832.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103559793.0</td>\n",
       "      <td>34348384.0</td>\n",
       "      <td>14761694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49110078.0</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>False</td>\n",
       "      <td>1060932.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>1617011.0</td>\n",
       "      <td>-3367011.0</td>\n",
       "      <td>6426990.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>7427621.0</td>\n",
       "      <td>86987.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17252530.0</td>\n",
       "      <td>10433518.0</td>\n",
       "      <td>4188667.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14622185.0</td>\n",
       "      <td>3275.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>242.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2979.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BELDEN TIMOTHY N</th>\n",
       "      <td>True</td>\n",
       "      <td>213999.0</td>\n",
       "      <td>5249999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2334434.0</td>\n",
       "      <td>2144013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210698.0</td>\n",
       "      <td>17355.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5501630.0</td>\n",
       "      <td>953136.0</td>\n",
       "      <td>157569.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1110705.0</td>\n",
       "      <td>7991.000000</td>\n",
       "      <td>484.000000</td>\n",
       "      <td>228.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>5521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>True</td>\n",
       "      <td>1111258.0</td>\n",
       "      <td>5600000.0</td>\n",
       "      <td>1920000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22122.0</td>\n",
       "      <td>29336.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8682716.0</td>\n",
       "      <td>19250000.0</td>\n",
       "      <td>6843672.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26093672.0</td>\n",
       "      <td>3627.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2042.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>False</td>\n",
       "      <td>267102.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1586055.0</td>\n",
       "      <td>-1386055.0</td>\n",
       "      <td>1295738.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2660303.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5634343.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>3942714.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10623258.0</td>\n",
       "      <td>2007.111111</td>\n",
       "      <td>668.763889</td>\n",
       "      <td>58.5</td>\n",
       "      <td>36.277778</td>\n",
       "      <td>1058.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAVORATO JOHN J</th>\n",
       "      <td>False</td>\n",
       "      <td>339288.0</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>2035380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>49537.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10425757.0</td>\n",
       "      <td>4158995.0</td>\n",
       "      <td>1008149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5167144.0</td>\n",
       "      <td>7259.000000</td>\n",
       "      <td>2585.000000</td>\n",
       "      <td>528.0</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>3962.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELAINEY DAVID W</th>\n",
       "      <td>True</td>\n",
       "      <td>365163.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>1294981.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>86174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4747979.0</td>\n",
       "      <td>2291113.0</td>\n",
       "      <td>1323148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3614261.0</td>\n",
       "      <td>3093.000000</td>\n",
       "      <td>3069.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>2097.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KEAN STEVEN J</th>\n",
       "      <td>False</td>\n",
       "      <td>404338.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>41953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1747522.0</td>\n",
       "      <td>2022048.0</td>\n",
       "      <td>4131594.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6153642.0</td>\n",
       "      <td>12754.000000</td>\n",
       "      <td>6759.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAEDICKE MARK E</th>\n",
       "      <td>False</td>\n",
       "      <td>374125.0</td>\n",
       "      <td>1150000.0</td>\n",
       "      <td>983346.0</td>\n",
       "      <td>-934484.0</td>\n",
       "      <td>2157527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52382.0</td>\n",
       "      <td>76169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3859065.0</td>\n",
       "      <td>608750.0</td>\n",
       "      <td>524169.0</td>\n",
       "      <td>-329825.0</td>\n",
       "      <td>803094.0</td>\n",
       "      <td>4009.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1847.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHALLEY LAWRENCE G</th>\n",
       "      <td>False</td>\n",
       "      <td>510364.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>808346.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301026.0</td>\n",
       "      <td>57838.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4677574.0</td>\n",
       "      <td>3282960.0</td>\n",
       "      <td>2796177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6079137.0</td>\n",
       "      <td>6019.000000</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>186.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3920.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      poi     salary      bonus  long_term_incentive  \\\n",
       "LAY KENNETH L        True  1072321.0  7000000.0            3600000.0   \n",
       "FREVERT MARK A      False  1060932.0  2000000.0            1617011.0   \n",
       "BELDEN TIMOTHY N     True   213999.0  5249999.0                  0.0   \n",
       "SKILLING JEFFREY K   True  1111258.0  5600000.0            1920000.0   \n",
       "BAXTER JOHN C       False   267102.0  1200000.0            1586055.0   \n",
       "LAVORATO JOHN J     False   339288.0  8000000.0            2035380.0   \n",
       "DELAINEY DAVID W     True   365163.0  3000000.0            1294981.0   \n",
       "KEAN STEVEN J       False   404338.0  1000000.0             300000.0   \n",
       "HAEDICKE MARK E     False   374125.0  1150000.0             983346.0   \n",
       "WHALLEY LAWRENCE G  False   510364.0  3000000.0             808346.0   \n",
       "\n",
       "                    deferred_income  deferral_payments  loan_advances  \\\n",
       "LAY KENNETH L             -300000.0           202911.0     81525000.0   \n",
       "FREVERT MARK A           -3367011.0          6426990.0      2000000.0   \n",
       "BELDEN TIMOTHY N         -2334434.0          2144013.0            0.0   \n",
       "SKILLING JEFFREY K              0.0                0.0            0.0   \n",
       "BAXTER JOHN C            -1386055.0          1295738.0            0.0   \n",
       "LAVORATO JOHN J                 0.0                0.0            0.0   \n",
       "DELAINEY DAVID W                0.0                0.0            0.0   \n",
       "KEAN STEVEN J                   0.0                0.0            0.0   \n",
       "HAEDICKE MARK E           -934484.0          2157527.0            0.0   \n",
       "WHALLEY LAWRENCE G              0.0                0.0            0.0   \n",
       "\n",
       "                         other  expenses  director_fees  total_payments  \\\n",
       "LAY KENNETH L       10359729.0   99832.0            0.0     103559793.0   \n",
       "FREVERT MARK A       7427621.0   86987.0            0.0      17252530.0   \n",
       "BELDEN TIMOTHY N      210698.0   17355.0            0.0       5501630.0   \n",
       "SKILLING JEFFREY K     22122.0   29336.0            0.0       8682716.0   \n",
       "BAXTER JOHN C        2660303.0   11200.0            0.0       5634343.0   \n",
       "LAVORATO JOHN J         1552.0   49537.0            0.0      10425757.0   \n",
       "DELAINEY DAVID W        1661.0   86174.0            0.0       4747979.0   \n",
       "KEAN STEVEN J           1231.0   41953.0            0.0       1747522.0   \n",
       "HAEDICKE MARK E        52382.0   76169.0            0.0       3859065.0   \n",
       "WHALLEY LAWRENCE G    301026.0   57838.0            0.0       4677574.0   \n",
       "\n",
       "                    exercised_stock_options  restricted_stock  \\\n",
       "LAY KENNETH L                    34348384.0        14761694.0   \n",
       "FREVERT MARK A                   10433518.0         4188667.0   \n",
       "BELDEN TIMOTHY N                   953136.0          157569.0   \n",
       "SKILLING JEFFREY K               19250000.0         6843672.0   \n",
       "BAXTER JOHN C                     6680544.0         3942714.0   \n",
       "LAVORATO JOHN J                   4158995.0         1008149.0   \n",
       "DELAINEY DAVID W                  2291113.0         1323148.0   \n",
       "KEAN STEVEN J                     2022048.0         4131594.0   \n",
       "HAEDICKE MARK E                    608750.0          524169.0   \n",
       "WHALLEY LAWRENCE G                3282960.0         2796177.0   \n",
       "\n",
       "                    restricted_stock_deferred  total_stock_value  \\\n",
       "LAY KENNETH L                             0.0         49110078.0   \n",
       "FREVERT MARK A                            0.0         14622185.0   \n",
       "BELDEN TIMOTHY N                          0.0          1110705.0   \n",
       "SKILLING JEFFREY K                        0.0         26093672.0   \n",
       "BAXTER JOHN C                             0.0         10623258.0   \n",
       "LAVORATO JOHN J                           0.0          5167144.0   \n",
       "DELAINEY DAVID W                          0.0          3614261.0   \n",
       "KEAN STEVEN J                             0.0          6153642.0   \n",
       "HAEDICKE MARK E                     -329825.0           803094.0   \n",
       "WHALLEY LAWRENCE G                        0.0          6079137.0   \n",
       "\n",
       "                     to_messages  from_messages  from_poi_to_this_person  \\\n",
       "LAY KENNETH L        4273.000000      36.000000                    123.0   \n",
       "FREVERT MARK A       3275.000000      21.000000                    242.0   \n",
       "BELDEN TIMOTHY N     7991.000000     484.000000                    228.0   \n",
       "SKILLING JEFFREY K   3627.000000     108.000000                     88.0   \n",
       "BAXTER JOHN C        2007.111111     668.763889                     58.5   \n",
       "LAVORATO JOHN J      7259.000000    2585.000000                    528.0   \n",
       "DELAINEY DAVID W     3093.000000    3069.000000                     66.0   \n",
       "KEAN STEVEN J       12754.000000    6759.000000                    140.0   \n",
       "HAEDICKE MARK E      4009.000000    1941.000000                    180.0   \n",
       "WHALLEY LAWRENCE G   6019.000000     556.000000                    186.0   \n",
       "\n",
       "                    from_this_person_to_poi  shared_receipt_with_poi  \n",
       "LAY KENNETH L                     16.000000              2411.000000  \n",
       "FREVERT MARK A                     6.000000              2979.000000  \n",
       "BELDEN TIMOTHY N                 108.000000              5521.000000  \n",
       "SKILLING JEFFREY K                30.000000              2042.000000  \n",
       "BAXTER JOHN C                     36.277778              1058.527778  \n",
       "LAVORATO JOHN J                  411.000000              3962.000000  \n",
       "DELAINEY DAVID W                 609.000000              2097.000000  \n",
       "KEAN STEVEN J                    387.000000              3639.000000  \n",
       "HAEDICKE MARK E                   61.000000              1847.000000  \n",
       "WHALLEY LAWRENCE G                24.000000              3920.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few considerations to make here:\n",
    "1. Kenneth Lay, [the CEO of Enron from 1986-2001](http://www.biography.com/people/kenneth-lay-234611), presided over many of the illegal business activites and hence is one of the most important persons of interest. \n",
    "2. Mark Frevert served as chief executive of [Enron Europe from 1986-2000 and was appointed as chairman of Enron in 2001](http://www.risk.net/risk-management/2123422/ten-years-after-its-collapse-enron-lives-energy-markets). He was a major player in the firm, although not a person of interest. I believe that he is not representative of the average employee at Enron during this time because of his substantial compensation and will remove him from the dataset. \n",
    "3. Timothy Belden was the [former head of trading for Enron](http://articles.latimes.com/2007/feb/15/business/fi-enron15) who developed the strategy to illegally raise energy prices in California. He was a person of interest and will definitely remain in the dataset. \n",
    "4. Jeffrey Skilling [replaced Kenneth Lay as CEO of Enron in 2001 and orchestrated much of the fraud](http://www.biography.com/people/jeffrey-skilling-235386) that destroyed Enron. As a person of interest, he will remain in the dataset. \n",
    "5. John Baxter was a former vice Enron vice chairman and [died of an apparent self-inflicted gunshot](https://www.wsws.org/en/articles/2002/01/enro-j28.html) before he was able to testify against other Enron executives. I will remove him from the dataset as he is not a person of interest. \n",
    "6. John Lavorato was a top executive in the energy-trading branch of Enron and received large bonuses to [keep him from leaving Enron](http://www.nytimes.com/2002/06/18/business/officials-got-a-windfall-before-enron-s-collapse.html). As he was not a person of interest, and the large bonus ended up skewing his total pay towards the top of the range, I think it would be appropriate to remove him from the dataset. \n",
    "4. Lawrence Whalley [served as the president of Enron](http://www.corpwatch.org/article.php?id=13194) and fired Andrew Fastow once it was apparent the severity of Enron's situation. He was investigated thoroughly but not identified as a person of interest and therefore will be removed from the dataset.  \n",
    "\n",
    "Total, that is four people to remove from the dataset. I believe these removals are justified primarily because none of these individuals were persons of interest and they all were upper-level executives with pay levels far above the average employee. I hesitate to remove any samples from the data, but I believe that removing these individuals will improve the quality of the classifier. I can try with and without removing these individuals and measure the accuracy, precision, and recall of the classifier to determine if my choice was justified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=['FREVERT MARK A', 'LAVORATO JOHN J', 'WHALLEY LAWRENCE G', 'BAXTER JOHN C'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    122\n",
       "True      18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df==0].count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 2800 observations of financial and email data in the set now that the data cleaning has been finished. Of these, __1150 or 41%__ are 0 financial values. There are 18 persons of interest, comprising __12.9%__ of the individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to begin training some classifiers with default parameters in order to identify existing features that are most predicative of persons of interest. After that, if the classifier performance is low, I will try and devise additional features and then fine-tune the algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Algorithm Testing\n",
    "\n",
    "The four algorithms I have selected for initial testing are Gaussian Naive Bayes (GaussianNB), DecisionTreeClassifier, Support Vector Classifier (SVC), and KMeans. I will run all of the algorithms with the default parameters except I will alter the kernel used in the Support Vector Machine to be linear. I will also select 2 to be the number of clusters for KMeans as I know in advance that there are two categories I want to identify. Although accuracy would seem to be the obvious choice for evaluating the quality of a classifier, accuracy can be a crude measure at times and is not suited for some datasets including this one. For example, if a classifier were to guess that all of the samples in my cleaned dataset were _not_ persons of interest, it would have an accuracy of 87.1%. However, this clearly would not satisfy the objective of this investigation which is to create a classifier that can identify persons of interest. Therefore, different metrics are needed to evaluate the tuned algorithm (a classifier is the algorithm plus the parameters selected) to gauge its effectiveness. The two selected for this project are [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "* __Precision__ is the number of correct positive results over the total number of positive labels assigned. In other words, it is the fraction of persons of interest predicted by the algorithm that are truly persons of interest.  Mathematically precision is defined as \n",
    "\n",
    "$$ precision = \\frac{true\\ positives}{true\\ positives + false\\ positives} $$ \n",
    "\n",
    "* __Recall__ is the number of correct positive results divided by the number of positive results that should have been identified. In other words, it is the fraction of the total number of persons of interest that the classifier correctly labels. Mathematically, recall is defined as\n",
    "\n",
    "$$ recall = \\frac{true\\ positives}{true\\ positives + false\\ negatives} $$ \n",
    "\n",
    "Precision is also known as positive predictive value while recall is the sensitivity of the classifier. A combined measured of precision and recall is the [__F1 score__](https://en.wikipedia.org/wiki/F1_score). Is it the harmonic mean of precision and recall. Mathematically, the F1 score is defined as:\n",
    "\n",
    "$$ F1\\ Score = \\frac{2\\ (precision\\ x\\ recall)}{precision + recall} $$\n",
    "\n",
    "For this project, the objective was a precision and a recall greater than 0.3. However, I believe it is possible to do much better than that with the right feature selection and algorithm tuning. \n",
    "\n",
    "The only preparation I will do for this initial testing of the different algorithms is to scale the data such that it has a zero mean and a unit variance. This process is called [normalization](http://www.analytictech.com/ba762/handouts/normalization.htm) and is accomplished using the scale function from the sklearn preprocessing module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.70714\tPrecision: 0.30909\tRecall: 0.85000\tF1: 0.45333\tF2: 0.62963\n",
      "\tTotal predictions:  140\tTrue positives:   17\tFalse positives:   38\tFalse negatives:    3\tTrue negatives:   82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from tester import test_classifier\n",
    "\n",
    "scaled_df = df.copy()\n",
    "scaled_df.ix[:,1:] = scale(scaled_df.ix[:,1:])\n",
    "data_dict = scaled_df.to_dict(orient='index')\n",
    "\n",
    "my_dataset = data_dict\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.82857\tPrecision: 0.40000\tRecall: 0.40000\tF1: 0.40000\tF2: 0.40000\n",
      "\tTotal predictions:  140\tTrue positives:    8\tFalse positives:   12\tFalse negatives:   12\tTrue negatives:  108\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04772727,  0.22272727,  0.19772727,  0.10738636,  0.        ,\n",
       "        0.        ,  0.02160393,  0.11853147,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.17897727,\n",
       "        0.        ,  0.        ,  0.        ,  0.10531915])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf = test_classifier(clf, my_dataset, features_list)\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'bonus',\n",
       " 'long_term_incentive',\n",
       " 'deferred_income',\n",
       " 'deferral_payments',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'expenses',\n",
       " 'director_fees',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'to_messages',\n",
       " 'from_messages',\n",
       " 'from_poi_to_this_person',\n",
       " 'from_this_person_to_poi',\n",
       " 'shared_receipt_with_poi']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.86429\tPrecision: 0.57143\tRecall: 0.20000\tF1: 0.29630\tF2: 0.22989\n",
      "\tTotal predictions:  140\tTrue positives:    4\tFalse positives:    3\tFalse negatives:   16\tTrue negatives:  117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.72143\tPrecision: 0.12000\tRecall: 0.15000\tF1: 0.13333\tF2: 0.14286\n",
      "\tTotal predictions:  140\tTrue positives:    3\tFalse positives:   22\tFalse negatives:   17\tTrue negatives:   98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from running the four classifiers on the entire featureset with no algorithm tuning are summarized in the table below\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.30909   | 0.85000 | 0.45333  | 0.70714  |\n",
    "| DecisionTree          | 0.5000    | 0.45000 | 0.47368  | 0.85714  |\n",
    "| SVC (kernel='linear') | 0.57143   | 0.2000  | 0.29630  | 0.86429  |\n",
    "| KMeans (n_clusters=2) | 0.17647   | 0.15000 | 0.16216  | 0.77857  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first run through the four algorithms, I can see that the decision tree performed best, followed by the gaussian naive bayes, support vector machine, and Kmeans clustering. In fact, the decision tree and naive Bayes classifiers both perform well enough to meet the standards for the project. Nonetheless, there is much work that can be done to improve these metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features\n",
    "\n",
    "The standard features for the dataset perform adequately, but not stellar. I want to define new features that will allow for more accurate predictions of poi. Additionally, I want to perform feature reduction using PCA in order to choose the dimensions that have the greatest variance and hopefully, predictive power. \n",
    "\n",
    "Three new features can be created from the emails at the moment. The first will be the ratio of emails to an individual from a person of interest to all emails addressed to that person, the second is the same but for messages to persons of interest, and the third will be the ratio of email receipts shared with a person of interest to all emails addressed to that individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['to_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['from_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "df['shared_poi_ratio'] = df['shared_receipt_with_poi'] / df['to_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list.append('to_poi_ratio')\n",
    "features_list.append('from_poi_ratio')\n",
    "features_list.append('shared_poi_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'bonus',\n",
       " 'long_term_incentive',\n",
       " 'deferred_income',\n",
       " 'deferral_payments',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'expenses',\n",
       " 'director_fees',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'to_messages',\n",
       " 'from_messages',\n",
       " 'from_poi_to_this_person',\n",
       " 'from_this_person_to_poi',\n",
       " 'shared_receipt_with_poi',\n",
       " 'to_poi_ratio',\n",
       " 'from_poi_ratio',\n",
       " 'shared_poi_ratio']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BELDEN TIMOTHY N                 5501630.0\n",
       "BOWEN JR RAYMOND M               2669589.0\n",
       "CALGER CHRISTOPHER F             1639297.0\n",
       "CAUSEY RICHARD A                 1868758.0\n",
       "COLWELL WESLEY                   1490344.0\n",
       "DELAINEY DAVID W                 4747979.0\n",
       "FASTOW ANDREW S                  2424083.0\n",
       "GLISAN JR BEN F                  1272284.0\n",
       "HANNON KEVIN P                    288682.0\n",
       "HIRKO JOSEPH                       91093.0\n",
       "KOENIG MARK E                    1587421.0\n",
       "KOPPER MICHAEL J                 2652612.0\n",
       "LAY KENNETH L                  103559793.0\n",
       "RICE KENNETH D                    505050.0\n",
       "RIEKER PAULA H                   1099100.0\n",
       "SHELBY REX                       2003885.0\n",
       "SKILLING JEFFREY K               8682716.0\n",
       "YEAGER F SCOTT                    360300.0\n",
       "ALLEN PHILLIP K                  4484442.0\n",
       "BADUM JAMES P                     182466.0\n",
       "BANNANTINE JAMES M                916197.0\n",
       "BAY FRANKLIN R                    827696.0\n",
       "BAZELIDES PHILIP J                860136.0\n",
       "BECK SALLY W                      969068.0\n",
       "BELFER ROBERT                       3285.0\n",
       "BERBERIAN DAVID                   228474.0\n",
       "BERGSIEKER RICHARD P              618850.0\n",
       "BHATNAGAR SANJAY                  137864.0\n",
       "BIBI PHILIPPE A                  2047593.0\n",
       "BLACHMAN JEREMY M                2014835.0\n",
       "                                  ...     \n",
       "POWERS WILLIAM                         0.0\n",
       "PRENTICE JAMES                    564348.0\n",
       "REDMOND BRIAN L                   111529.0\n",
       "REYNOLDS LAWRENCE                 394475.0\n",
       "SAVAGE FRANK                        3750.0\n",
       "SCRIMSHAW MATTHEW                      0.0\n",
       "SHANKMAN JEFFREY A               3038702.0\n",
       "SHAPIRO RICHARD S                1057548.0\n",
       "SHARP VICTORIA T                 1576511.0\n",
       "SHERRICK JEFFREY B                     0.0\n",
       "SHERRIFF JOHN R                  4335388.0\n",
       "STABLER FRANK                    1112087.0\n",
       "SULLIVAN-SHAKLOVITZ COLLEEN       999356.0\n",
       "SUNDE MARTIN                     1545059.0\n",
       "TAYLOR MITCHELL S                1092663.0\n",
       "THORN TERENCE H                   911453.0\n",
       "TILNEY ELIZABETH A                399393.0\n",
       "UMANOFF ADAM S                   1130461.0\n",
       "URQUHART JOHN A                   228656.0\n",
       "WAKEHAM JOHN                      213071.0\n",
       "WALLS JR ROBERT H                1798780.0\n",
       "WALTERS GARETH W                   87410.0\n",
       "WASAFF GEORGE                    1034395.0\n",
       "WESTFAHL RICHARD K                762135.0\n",
       "WHALEY DAVID A                         0.0\n",
       "WHITE JR THOMAS E                1934359.0\n",
       "WINOKUR JR. HERBERT S              84992.0\n",
       "WODRASKA JOHN                     189583.0\n",
       "WROBEL BRUCE                           0.0\n",
       "YEAP SOON                          55097.0\n",
       "Name: total_payments, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_payments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I will also create some new features using the financial data. I have a few theories that I formed from my initial data exploration and reading about the Enron case. I think that people recieving large bonuses may be more likely to be persons of interest becuase the bonuses could be a result of fraudelent activity, or perhaps a bribe to keep someone quiet. Whatever the case may be, I will create two new features that are the bonus in relation to the salary, and the bonus in relation to total payments. I am creating lots of extra features at this point and now have a total of 27. However, I will perform feature reduction/selection eventually so I am not worried about the large number of features. Moreover the algorithms I am using are able to train relatively quickly even with the large number of features because the total amount of data samples is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['bonus_to_salary'] = df['bonus'] / df['salary']\n",
    "df['bonus_to_total'] = df['bonus'] / df['total_payments']\n",
    "features_list.append('bonus_to_salary')\n",
    "features_list.append('bonus_to_total')\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_classif, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.72857\tPrecision: 0.32000\tRecall: 0.80000\tF1: 0.45714\tF2: 0.61538\n",
      "\tTotal predictions:  140\tTrue positives:   16\tFalse positives:   34\tFalse negatives:    4\tTrue negatives:   86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill any NaN financial data with a 0\n",
    "df.fillna(value= 0, inplace=True)\n",
    "\n",
    "# Create a copy of the dataframe and normalize it to zero mean and unit variance\n",
    "scaled_df = df.copy()\n",
    "scaled_df.ix[:,1:] = scale(scaled_df.ix[:,1:])\n",
    "data_dict = scaled_df.to_dict(orient='index')\n",
    "\n",
    "# \n",
    "my_dataset = data_dict\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "clf = test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.90714\tPrecision: 0.68421\tRecall: 0.65000\tF1: 0.66667\tF2: 0.65657\n",
      "\tTotal predictions:  140\tTrue positives:   13\tFalse positives:    6\tFalse negatives:    7\tTrue negatives:  114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf = test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.83571\tPrecision: 0.33333\tRecall: 0.15000\tF1: 0.20690\tF2: 0.16854\n",
      "\tTotal predictions:  140\tTrue positives:    3\tFalse positives:    6\tFalse negatives:   17\tTrue negatives:  114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf = test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.71429\tPrecision: 0.16667\tRecall: 0.25000\tF1: 0.20000\tF2: 0.22727\n",
      "\tTotal predictions:  140\tTrue positives:    5\tFalse positives:   25\tFalse negatives:   15\tTrue negatives:   95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "clf = test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding in the five additional features, I retested the algorithms with all of the features. The results are summarized in the table below.\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.35556   | 0.80000 | 0.49321  | 0.76429  |\n",
    "| DecisionTree          | 0.60000   | 0.60000 | 0.60000  | 0.88571  |\n",
    "| SVC (kernel='linear') | 0.71429 | 0.25000 | 0.37037  | 0.87857  |\n",
    "| KMeans (n_clusters=2) | 0.12500   | 0.25000 | 0.16667  | 0.64286  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 Score improved for all four of the classifiers with the addition of the two additional created features. The F1 score for the decision tree is stil the highest followed by the Gaussian Naive Bayes. At this point, neither the SVC with the linear kernel nor the KMeans clustering pass the standards of 0.3 for precision and recall. I will drop the latter two algorithms and I will also drop the GaussianNB in favor of [AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf) because it has GaussianNB does not have any tunable parameters and therefore I will not be able to improve the precision or recall any beyond what I do to alter the features. AdaBoost fits multiple classifiers on a dataset and adjusts the weights of incorrectly classified instances with each iteration to concentrate on the difficult to classify samples. \n",
    "\n",
    "The next step I want to take is feature selection. Looking at the features importances for both AdaBoost and DecisionTree, I can see that there are some features that have a zero importance. I will use selectKbest to select the best performing features and use gridSearchCV to pick the optimum number of features to select. I will also add one more feature, shared_ratio, or the ratio of messags an indidual recieved that were shared with a person of interest divided by the total number of emails received by that individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I have 25 features. I know that not all of them are going to be contributing to the accuracy of my model. Therefore, it is time to perform dimensionality reduction. First, I will simply use the .feature_importances\\_ attribute of the DecisionTreeClassifier and the AdaBoostClassifier to determine the most important features for each algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.90714\tPrecision: 0.68421\tRecall: 0.65000\tF1: 0.66667\tF2: 0.65657\n",
      "\tTotal predictions:  140\tTrue positives:   13\tFalse positives:    6\tFalse negatives:    7\tTrue negatives:  114\n",
      "\n",
      "to_poi_ratio : 0.3782\n",
      "from_this_person_to_poi : 0.2485\n",
      "other : 0.2476\n",
      "loan_advances : 0.0665\n",
      "shared_receipt_with_poi : 0.0592\n",
      "poi : 0.0000\n",
      "salary : 0.0000\n",
      "bonus : 0.0000\n",
      "long_term_incentive : 0.0000\n",
      "deferred_income : 0.0000\n"
     ]
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree = test_classifier(clf_tree, my_dataset, features_list)\n",
    "\n",
    "tree_feature_importances = [0] + (clf_tree.feature_importances_)\n",
    "tree_features = zip(tree_feature_importances, features_list)\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.89286\tPrecision: 0.63158\tRecall: 0.60000\tF1: 0.61538\tF2: 0.60606\n",
      "\tTotal predictions:  140\tTrue positives:   12\tFalse positives:    7\tFalse negatives:    8\tTrue negatives:  113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(clf_tree, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.94286\tPrecision: 0.83333\tRecall: 0.75000\tF1: 0.78947\tF2: 0.76531\n",
      "\tTotal predictions:  140\tTrue positives:   15\tFalse positives:    3\tFalse negatives:    5\tTrue negatives:  117\n",
      "\n",
      "from_this_person_to_poi : 0.1200\n",
      "total_payments : 0.1000\n",
      "from_poi_to_this_person : 0.1000\n",
      "shared_receipt_with_poi : 0.1000\n",
      "long_term_incentive : 0.0800\n",
      "to_messages : 0.0800\n",
      "to_poi_ratio : 0.0800\n",
      "loan_advances : 0.0600\n",
      "restricted_stock_deferred : 0.0600\n",
      "from_poi_ratio : 0.0400\n"
     ]
    }
   ],
   "source": [
    "clf_ada = AdaBoostClassifier()\n",
    "clf_ada = test_classifier(clf_ada, my_dataset, features_list)\n",
    "ada_feature_importances = [0] + clf_ada.feature_importances_\n",
    "ada_features = zip(ada_feature_importances, features_list)\n",
    "for pair in ada_features:\n",
    "    importance = round(pair[0],4)\n",
    "    if importance == 0.0:\n",
    "        ada_features.remove(pair)\n",
    "        \n",
    "ada_features = sorted(ada_features, key=lambda x:x[0], reverse=True)\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(ada_features[i][1], ada_features[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is interesting to compare the feature importances for the DecisionTree and the AdaBoost classifiers. They definitely are not in close agreement for the top ten even though both manage a respectable F1 Score greater than 0.5. The next step is to make a pipeline and then let GridSearchCV do the tough work of selecting the optimal number of features to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_dict = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data_dict)\n",
    "\n",
    "cv = StratifiedShuffleSplit(random_state = 42)\n",
    "for train_idx, test_idx in cv.split(features, labels): \n",
    "    features_train = []\n",
    "    features_test  = []\n",
    "    labels_train   = []\n",
    "    labels_test    = []\n",
    "    for ii in train_idx:\n",
    "        features_train.append( features[ii] )\n",
    "        labels_train.append( labels[ii] )\n",
    "    for jj in test_idx:\n",
    "        features_test.append( features[jj] )\n",
    "        labels_test.append( labels[jj] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will Koehrsen\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=10, score_func=<function f_classif at 0x000000000C4F0D68>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'select_features__k': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_features = np.arange(1, len(features_list))\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "tree_clf= GridSearchCV(pipe, param_grid=param_grid, scoring='f1', cv = 10)\n",
    "tree_clf.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62959183673469388"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 19}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the grid search performed with SelectKBest with the number of features ranging from 1 to the (number of features - 1), the optimal number of features for the decision tree classifier is 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.91429\tPrecision: 0.68182\tRecall: 0.75000\tF1: 0.71429\tF2: 0.73529\n",
      "\tTotal predictions:  140\tTrue positives:   15\tFalse positives:    7\tFalse negatives:    5\tTrue negatives:  113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the DecisionTreeClassifier with SelectKMeans and k = 20 yields an F1 score of 0.71429. I am very pleased with that result but will still aim higher. I have not forgotten about the AdaBoostClassifier and I will try to reduce the number of features for that algorithm in the same manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=10, score_func=<function f_classif at 0x000000000C4F0D68>)), ('classify', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'select_features__k': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = np.arange(1, len(features_list))\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "ada_clf= GridSearchCV(pipe, param_grid=param_grid, scoring='f1')\n",
    "ada_clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 20}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68994708994708986"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.90000\tPrecision: 0.66667\tRecall: 0.60000\tF1: 0.63158\tF2: 0.61224\n",
      "\tTotal predictions:  140\tTrue positives:   12\tFalse positives:    6\tFalse negatives:    8\tTrue negatives:  114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(ada_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal number of parameters found by the GridSearch for the AdaBoostClassifier was also 20. This resulted in a slightly lower F1 score of 0.689, but still quite high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I could also perform Principal Component Analysis, but I think that the performance I am seeing is high enough. PCA creates new features that do not necessarily represent actual quantifiable values in the dataset, and I like the idea that I know exactly all the features I am putting into the model. This is one way that I try to combat the [black box problem](http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731) in machine learning. If I at least know what is going in, then I can try to understand why the model returned a certain classification and maybe it can inform my thinking for future machine learning systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to begin tuning the classifiers. I will use GridSearchCV again and I will input a wide variety of parameters to try in the parameter grid. The decision tree classifier will be up first. Looking at the sci-kit learn [documentation for the DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), there are a number of parameters that can be tuned. However, I will focus on just a few. The first will be __criterion__ for splitting, either 'gini', or 'entropy' to maximize the [information gain](https://en.wikipedia.org/wiki/Information_gain_ratio). The other three I will try will be __min_samples_split__, __max_depth__, and __max_features__. I will continue to use a cross-validation with 10 folds in the grid search and the scoring criterion will remain set at F1 because that is what I am trying to maximize.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x000000000C4F0D68>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'classify__max_features': [None, 'sqrt', 'log2', 'auto'], 'classify__min_samples_split': [2, 4, 6, 8, 10], 'classify__criterion': ['gini', 'entropy'], 'classify__max_depth': [None, 5, 10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_pipe = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = dict(classify__criterion = ['gini', 'entropy'] , \n",
    "                  classify__min_samples_split = [2, 4, 6, 8, 10],\n",
    "                  classify__max_depth = [None, 5, 10, 15, 20],\n",
    "                  classify__max_features = [None, 'sqrt', 'log2', 'auto'])\n",
    "\n",
    "tree_clf = GridSearchCV(tree_pipe, param_grid = param_grid, scoring='f1', cv=10)\n",
    "tree_clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82645502645502644"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify__criterion': 'entropy',\n",
       " 'classify__max_depth': None,\n",
       " 'classify__max_features': None,\n",
       " 'classify__min_samples_split': 8}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now implement the best parameters selected by the Grid Search and cross validation them using the tester.py function. I am seeing quite high F1 scores, and I am wondering if perhaps I am making some large mistake, such as training on the same data that I test on. I don't think this is so because I am using cross validation in the GridSearch, but it only takes a few seconds to test so I might as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.94286\tPrecision: 1.00000\tRecall: 0.60000\tF1: 0.75000\tF2: 0.65217\n",
      "\tTotal predictions:  140\tTrue positives:   12\tFalse positives:    0\tFalse negatives:    8\tTrue negatives:  120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=20)),\n",
    "    ('classify', DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features=None, min_samples_split=10))\n",
    "])\n",
    "\n",
    "dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the cross validation in tester.py, my F1 score is indeed very high with the recommended parameters. Now, I am satisfied with the recall and precision score of the DecisionTreeClassifier. I will play around with the AdaBoostClassifier using the same approach because I am genuinely curious to see if I can beat an F1 score of 0.767. It's time to look at the Sci-kit learn [documentation for the AdaBoost Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) to see the parameters available to tune. Again, the AdaBoost classifier works by iterating with several simpler classifiers and adjusting the weights given to each feature based on whether it labeled the sample correctly or not. The weights are adjusted with each iteration to improve the overall classifier with time. I do not know much about this classifer, but I can read the documentation and automate the optimal selection of parameters using GridSearch Cross Validation. \n",
    "\n",
    "The AdaBoostClassifier boosts another 'base' classifier, which by default is the Decision Tree. I can alter this using the __base_estimator__ parameter to tree out a random forest and the gaussian naive bayes classification. Ideally, AdaBoost is used on weak classifiers, or those that perform only slightly better than random. One example would be a [decision stump](http://stackoverflow.com/questions/12097155/weak-classifier) or a decision tree with only a single layer. However, I will try stronger classifiers for the base as well. The other parameters I can change are __n_estimators__ which is how many weak models to fit and __learning_rate__, a measure of the weight given to each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_pipe = Pipeline([('select_features', SelectKBest(k=20)),\n",
    "                     ('classify', AdaBoostClassifier())\n",
    "                    ])\n",
    "\n",
    "param_grid = dict(classify__base_estimator=[DecisionTreeClassifier(), RandomForestClassifier(), GaussianNB()],\n",
    "                  classify__n_estimators = [30, 50, 70, 120],\n",
    "                  classify__learning_rate = [0.5, 1, 1.5, 2, 4])\n",
    "\n",
    "ada_clf = GridSearchCV(ada_pipe, param_grid=param_grid, scoring='f1', cv=10)\n",
    "ada_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
